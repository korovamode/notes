# AI Assistants and the Drift Into Dependency

Korovamode | January 9, 2026

> **Note:** This is a short edition. Based on the full paper published December 28, 2025: *[The Augmented Self: AI Scaffolds, Offloading, and the Drift Toward Dependency](https://doi.org/10.5281/zenodo.18079615)*

A subtle change is underway in how knowledge work begins. More and more, the first coherent version of a thought arrives already shaped—quickly, fluently, and with plausible next steps attached. This can feel like simple convenience. But when the starting point changes, the rest of the workflow changes with it: what gets practiced, what feels effortful, and what counts as “normal” speed and competence. What follows describes that shift at the level of everyday work and explains why its effects are easiest to see when the tool is unavailable.


### Assistance Moved Upstream
Earlier productivity tools mostly supported execution: formatting, retrieval, transcription, or polish. Today’s assistants participate earlier, supplying a coherent first pass on meaning and direction. Instead of only helping you say what you already know, they can propose what the situation *is*, what matters within it, and what to do next. The work still ends with a human decision, but the starting point is more often a generated draft, plan, or stance that arrives already shaped.

An **intermediate cognition layer** is now available on demand: a quick external first pass that sits between raw input and a finished output, turning ambiguity into something workable—an outline, a draft reply, an action list, a provisional framing. In that role, it functions as a **scaffold**: a support layer that makes work easier while it is present, and reveals its role when it is removed. A simple version of the pattern is familiar: you receive a dense or delicate message, ask for a reply, get a coherent candidate with implied intent and next steps, then revise and send. The result can be fluent even when some of the earliest interpretive work has been partially externalized.

That matters because “starting” is where uncertainty is highest and where framing decisions quietly determine what counts as relevant, what gets excluded, and what seems like a reasonable next step. When this upstream layer becomes reliable and ubiquitous, workflows reorganize around it because it becomes the easiest way to move from ambiguity to coherence.


### From Originator to Editor
The most visible interaction with an assistant is revision: you read a draft, adjust it, and decide what to keep. Over time, that can mask a deeper change: the initial framing and first wording are increasingly supplied externally. In **originator mode**, you generate the first frame—what the thing is, what it’s for, what constraints matter—then build outward from that foundation. In **editor mode**, you begin with **suggested options**: candidate framings, outlines, messages, or action lists that arrive already shaped. Editing can be active and thoughtful, but it is not the same skill as originating under uncertainty. The shift is easy to miss because the visible labor (revising) remains while the invisible labor (forming the starting point) thins.

Two mechanisms explain why this shift has lasting effects. **Offloading** is what gets delegated: not just retrieval or drafting, but intermediate cognition—interpretation, framing, formulation, and sometimes checking. **Mediation** is how the assistant shapes outcomes by structuring the option set: the outputs are **suggested options** that compress the space of possible framings into a small menu of fluent candidates. Even when a user remains in control, the shape of control changes: judgment increasingly operates over pre-formed candidates rather than forming the candidate space itself.


### The Slow Consequence of Drift
The central concern is **drift**: gradual change in what gets practiced (and what becomes effortful) when the first pass is routinely externalized. Drift is not a single failure. It is a slow redistribution of attention and effort across the workflow. Day-to-day output can improve, even as certain upstream capacities become less exercised and less reliable on demand.

At the level of *what the situation is taken to be*, a subtle **interpretation drift** can set in. When an assistant regularly provides the first coherent reading—what matters, what the intent is, what the constraints probably are—your own initial pass can compress or disappear. Evaluation may still occur, but it begins downstream of a premade interpretation. Over time, the skill of generating multiple plausible readings from sparse evidence can weaken, and the default becomes accepting or lightly adjusting a provided frame.

**Formulation drift** appears when ambiguity is converted into structure by default. Drafts, outlines, plans, and “reasonable next steps” arrive pre-shaped, and the work becomes selection and revision. Editing can remain strong (and can even improve), but it is not the same as originating: choosing a structure from scratch, inventing the first phrasing under uncertainty, or building an argument before a template exists. When a workflow relies on externally provided first drafts, “starting from zero” becomes less familiar, and therefore feels slower and more cognitively costly.

Checking changes too, and the shift is often best described as **verification drift**. Fluent output carries signals of completeness: it looks finished, balanced, and confident. That can reduce the felt need to verify assumptions, trace sources, or test edge cases—especially when the task is time-pressured or the topic is unfamiliar. The risk is not only factual error. It is upstream misalignment: a mistaken assumption about context, an omitted constraint, an overconfident inference, or a prematurely narrowed frame that quietly propagates through everything that follows. In such cases, coherence becomes a proxy for correctness, and “seems done” becomes a stopping rule.


### Interruption & Normalization
Dependency is most legible under interruption. When access is constrained—by outage, policy, cost, latency, or context—the friction does not primarily appear at the end of a task. It appears upstream, where the scaffold had been turning uncertainty into an initial structure. What breaks first is often the “start”: forming a frame, choosing a stance, generating a plan, or deciding what to verify. In this sense, dependency can be described by **removability**: what changes, and where the workflow fails, when the scaffold is absent. The question is not whether the workflow can continue at all, but how its resilience changes when the intermediate cognition layer is removed.

As scaffolding becomes common, expectations adapt. When fast coherence and high-quality drafts are readily available, they begin to define the baseline of normal performance. Timelines, review cycles, and the perceived “reasonable” speed of communication can shift toward the assumption that a first pass is always immediately obtainable. Over time, opting out can look like slowness rather than a different mode of work.


### Agency & Authorship
An assistant can be a genuine extension of capability. It can also become the default place where “starting” happens—where uncertainty is converted into coherence and the candidate space of meanings and actions is quietly shaped. The point is not to deny the value of scaffolding, but to notice what it relocates: interpretation, framing, and first-pass work. If judgment increasingly operates on fluent options that arrive already formed, what becomes of agency and authorship—and how do we keep that shift legible as it becomes normal?


#korovamode


---

### Selected References

- Andy Clark and David J. Chalmers, “The Extended Mind,” *Analysis* 58, no. 1 (1998): 7–19. https://doi.org/10.1093/analys/58.1.7
- Amos Tversky and Daniel Kahneman, “The Framing of Decisions and the Psychology of Choice,” *Science* 211, no. 4481 (1981): 453–458. https://doi.org/10.1126/science.7455683
- Evan F. Risko and Sam J. Gilbert, “Cognitive Offloading,” *Trends in Cognitive Sciences* 20, no. 9 (2016): 676–688. https://doi.org/10.1016/j.tics.2016.07.002
- John D. Lee and Katrina A. See, “Trust in Automation: Designing for Appropriate Reliance,” *Human Factors* 46, no. 1 (2004): 50–80. https://doi.org/10.1518/hfes.46.1.50_30392
- Raja Parasuraman and Victor Riley, “Humans and Automation: Use, Misuse, Disuse, Abuse,” *Human Factors* 39, no. 2 (1997): 230–253. https://doi.org/10.1518/001872097778543886
- Shakked Noy and Whitney Zhang, “Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence,” *Science* 381, no. 6654 (2023): 187–192. https://doi.org/10.1126/science.adh2586
- Erik Brynjolfsson, Danielle Li, and Lindsey Raymond, “Generative AI at Work,” *The Quarterly Journal of Economics* 140, no. 2 (May 2025): 889–942. https://doi.org/10.1093/qje/qjae044


**Full version:** *[The Augmented Self: AI Scaffolds, Offloading, and the Drift Toward Dependency](https://doi.org/10.5281/zenodo.18079615)* (Korovamode).
