# La nouvelle machinerie de la persuasion
### IA générative, architecture d’influence et pilotage discret de la pensée

Korovamode | 26 novembre 2025


## 1. Introduction — De la persuasion de masse à l’architecture d’influence

La plupart des jours désormais, des millions de personnes ouvrent une fenêtre de discussion et demandent à un système d’IA de résumer un article, de rédiger un e-mail, de réécrire une politique, d’expliquer une décision de justice, de relire un contrat ou de donner des conseils sur le travail, les relations et la politique. Les réponses arrivent sur le ton d’un tuteur patient ou d’un collègue : pas de gros titres criards, pas de bannières, pas de propagande évidente.

Mais la question reste la même que celle qui a hanté tous les systèmes d’information du siècle dernier :  
**Qui décide de ce qui paraît raisonnable, normal ou inévitable ?**

L’IA générative a créé une nouvelle machinerie de persuasion : des agencements à grande échelle de médias et de calcul qui façonnent la manière dont les personnes rencontrent l’information et forment leurs jugements. Au sein de cette machinerie plus vaste, cette analyse se concentre sur une **architecture d’influence** : une configuration spécifique de données, d’interfaces et d’habitudes de dépendance intime par lesquelles les systèmes génératifs co-rédigent silencieusement le raisonnement quotidien.

Les premiers critiques des médias se préoccupaient déjà de questions similaires à l’ère de la diffusion de masse. Walter Lippmann soutenait que, dans des sociétés vastes et complexes, les citoyennes et citoyens ordinaires ne peuvent pas rencontrer directement la réalité ; ils doivent s’en remettre à des images médiatisées du monde, qui peuvent être organisées de manière à produire du consentement plutôt qu’à simplement informer.[1] Plus tard, Edward S. Herman et Noam Chomsky ont décrit comment les organisations de presse, contraintes par la structure de propriété, la publicité, les routines de collecte de l’information et les présupposés idéologiques, produisaient ce qu’ils ont appelé **la fabrication du consentement** : un consensus piloté dans lequel certaines versions de la réalité paraissent naturelles, tandis que d’autres n’apparaissent presque jamais.[2]

L’essor des **médias-plateformes** a changé la machinerie, mais non l’ambition. Au lieu de quelques chaînes de diffusion, les fils des réseaux sociaux, les moteurs de recherche et les systèmes de recommandation ont commencé à personnaliser l’attention. Ils ont suivi les clics, les partages et le temps de visionnage, puis ont appris quelles combinaisons de contenu, de contexte et de timing étaient les plus susceptibles de maintenir l’engagement des personnes. La persuasion n’était plus seulement une question de ce qui apparaissait à la une ou au journal télévisé du soir ; elle est devenue une question de la façon dont les fils étaient ajustés à chaque utilisateur et de la manière dont ces fils encadraient ce qui comptait comme pertinent, urgent ou crédible.

L’IA générative va un pas plus loin. Un grand modèle de langage ne se contente pas de classer et de recommander des messages existants. Il **synthétise de nouveaux textes, images, codes et arguments à la demande**, en s’appuyant sur un monde statistique appris pour produire des continuations plausibles de ce qu’un utilisateur a commencé. Lorsqu’une personne demande au système « explique cet article », « rédige une réponse » ou « aide-moi à peser le pour et le contre », le modèle ne fait pas que récupérer de l’information. Il propose un **chemin de pensées** qui relie un problème à la solution qu’il avance.

Ce schéma peut être décrit comme un **pilotage cognitif**. Lorsqu’un système d’IA contribue à structurer la manière dont les questions sont posées, quelles distinctions comptent, quels arbitrages sont mis en avant et quels exemples servent à illustrer un point, il participe directement au processus même de formation des jugements. Le système se présente comme une aide neutre, bien avant que quiconque n’aboutisse à une opinion explicite.

Cet essai utilise l’expression **architecture d’influence** pour désigner l’agencement en couches qui rend possible un tel pilotage, et **pilotage cognitif** pour son effet sur les schémas de pensée. Cette architecture fonctionne à travers trois couches. À la **couche des données**, les modèles apprennent une image statistique du monde à partir de vastes collections de textes et d’autres médias. À la **couche d’interface**, les politiques, les invites et la conception des produits décident quelles parties de ce monde interne sont exprimées, avec quel ton et quel cadrage. À la **couche d’intimité**, les systèmes instaurent, au fil du temps, confiance, familiarité et habitudes de dépendance, à mesure que les personnes reviennent vers eux dans des moments d’incertitude, de pression ou de doute de soi.

Au sein de cette architecture, des visions du monde particulières, des logiques institutionnelles et des stratégies de marque peuvent être intégrées comme paramètres par défaut : non seulement dans les bannières et les publicités, mais aussi dans la manière dont les problèmes sont interprétés, les options sont organisées et le réconfort est offert. Des récits et des jugements de valeur peuvent être insérés dans des explications, des recommandations et de petits gestes d’assistance au quotidien.

L’analyse qui suit traite l’IA générative comme une nouvelle machinerie de persuasion organisée autour de cette architecture d’influence. La section 2 décrit plus en détail les trois couches — données, interface, intimité — et montre comment elles rendent possible le pilotage cognitif et l’**enchâssement narratif**. La section 3 suit la manière dont cette architecture est déjà déployée dans l’image de marque des entreprises et la persuasion politique. La section 4 se tourne vers les routines ordinaires de recherche, de travail et de construction de soi dans lesquelles les personnes raisonnent désormais *à l’intérieur* de ces systèmes. La section 5 revient enfin aux questions de gouvernance et de **souveraineté cognitive**, en demandant ce que signifie, pour des individus, penser et décider au sein d’infrastructures qui co-rédigent silencieusement leur raisonnement quotidien.


## 2. L’architecture d’influence — Comment les systèmes d’IA orientent la pensée

L’IA générative n’oriente pas la pensée par un mécanisme unique. Elle fonctionne par empilement de décisions : quels textes sont utilisés pour entraîner le modèle, comment les questions sont interprétées et auxquelles il est répondu, et comment le système se présente lui-même comme assistant ou compagnon. Ensemble, ces couches forment une architecture d’influence qui façonne discrètement la manière dont les problèmes sont cadrés et quelles réponses paraissent naturelles.

Dans cet essai, *machinerie de la persuasion* désigne les grands systèmes médiatiques qui organisent l’attention et l’interprétation, et *architecture d’influence* l’agencement particulier de données, d’interfaces et d’intimité par lequel l’IA générative accomplit désormais ce travail.

Cette section retrace cette architecture à travers trois couches : **données**, **interface** et **intimité**.

### 2.1 La couche des données : entraîner un monde statistique

Les grands modèles de langage sont parfois décrits comme s’ils « répétaient simplement leurs données d’entraînement ». En réalité, ils apprennent un monde statistique dense : un modèle qui approxime quels mots, quelles tournures et quelles configurations ont tendance à apparaître ensemble, dans quels contextes, avec quelles connotations et associations. Ils n’apprennent pas une carte transparente de la réalité, mais un paysage de continuations probables.

On peut décrire ce paysage statistique comme un **champ de probabilité**. Au lieu d’énoncés isolés, le modèle apprend des gradients : certaines réponses, associations et styles de raisonnement deviennent plus probables que d’autres, compte tenu de l’immense corpus sur lequel il a été entraîné. Lorsqu’un utilisateur saisit une invite, le modèle échantillonne dans ce champ – en choisissant, étape par étape, les continuations que le système juge les plus probables.

Trois processus sont particulièrement importants pour la persuasion.

Le premier concerne la **sélection des données**. Les modèles sont entraînés sur des mélanges massifs de pages web, de livres, d’articles scientifiques, de code, de forums, de sous-titres et d’autres archives. Ce mélange n’est pas neutre. Il reflète l’accès à certaines sources, les décisions de web scraping et de licences, et de nombreuses étapes de filtrage visant à supprimer le spam, les contenus illégaux ou certains registres de langage. Chaque étape – quels sites sont indexés, quels livres sont numérisés, quels corpus sont surreprésentés, quels dialectes et registres sont jugés « de qualité suffisante » – contribue à façonner le monde statistique que le modèle internalise.[3][4][5]

Le deuxième processus concerne la **curation et la mise en forme** de ce monde. Les équipes peuvent augmenter les données avec des ensembles « de haute qualité » : démonstrations rédigées par des annotateurs, dialogues exemplaires, explications étape par étape et « bonnes réponses » à des questions types. Elles peuvent également filtrer ou down-weighter certains types de contenus : discours haineux, spam, mais aussi formes de contestation ou de langage jugées peu souhaitables du point de vue de la marque ou du produit. Ces décisions ne se contentent pas de nettoyer les données ; elles renforcent certaines manières de parler, de catégoriser et d’expliquer, au détriment d’autres.

Le troisième processus est l’**affinage ciblé**, qui inclut le fine-tuning et la recherche augmentée (RAG). Les modèles peuvent être adaptés sur des corpus spécialisés – documents juridiques, dossiers médicaux, contenus de support client, connaissances internes – de sorte que certaines voix deviennent beaucoup plus influentes dans les réponses du système.[6] Lorsque l’IA utilise la recherche augmentée, le choix des documents de référence, leurs résumés et la manière dont ils sont insérés dans le contexte influent directement sur ce qu’un utilisateur verra comme « les faits pertinents » ou « les arguments évidents ».

En parallèle, des techniques d’**apprentissage par renforcement à partir de retours humains** (RLHF) et d’autres formes d’alignement sont utilisées pour rendre les systèmes « utiles, inoffensifs et honnêtes ».[7][8] Des annotateurs évaluent des paires de réponses, classent des sorties comme acceptables ou inacceptables, ou écrivent des démonstrations dont le modèle doit s’inspirer. Au fil du temps, cela façonne le champ de probabilité : certaines formulations, registres et prises de position deviennent plus probables ; d’autres sont supprimées ou reformulées avant d’apparaître.

Les opérations sur les données ne se limitent pas à des aspects techniques ; elles encodent des **normes, intérêts et cadres institutionnels** dans le monde statistique du modèle. Si les corpus surreprésentent certains médias, régions, idéologies ou styles de gestion, ces perspectives deviennent les plus naturelles ou « plausibles » dans les réponses. Si des systèmes sont ensuite réentraînés sur leurs propres sorties, ou sur des moteurs de recherche déjà influencés par des campagnes de désinformation, ces biais peuvent être amplifiés. Des enquêtes ont déjà montré comment des réseaux coordonnés de sites peuvent inonder les résultats avec des contenus traduits automatiquement qui promeuvent des récits spécifiques, par exemple des messages pro-Kremlin ; des systèmes d’IA qui s’appuient sur ces résultats risquent de reprendre ces récits comme toile de fond « normale » de leurs réponses.[9]

La couche des données ne fixe pas un message unique. Elle façonne un espace des possibles : un champ de probabilité dans lequel certains récits, métaphores, cadres et styles de raisonnement deviennent simplement plus faciles à produire que d’autres.

### 2.2 La couche d’interface : cadrer les questions et les réponses

La deuxième couche de l’architecture d’influence est l’**interface** – la manière dont les systèmes sont intégrés dans des produits, des invites système, des flux de travail et des modèles d’interaction. Même si deux modèles sous-jacents sont identiques, leur apparence pour les utilisateurs – ce qu’ils « semblent » faire – dépend fortement de cette couche.

Les produits définissent d’abord des **rôles et des métaphores** : assistant, copilote, coach, tuteur, conseiller juridique « non contraignant », soutien à la santé mentale, et ainsi de suite. Ces métaphores ne sont pas de simples accessoires marketing. Elles signalent à quoi le système doit servir, quel type de relation est attendu et quelles questions il est approprié de poser. Elles influencent aussi le ton des réponses : chaleureux et empathique, neutre et bureaucratique, ou orienté optimisation et performance.

Ensuite, les concepteurs formulent des **prompts système**, des règles d’utilisation et des politiques de sécurité qui guident la manière dont les questions sont interprétées. Ces instructions internes peuvent dire au modèle d’éviter certains sujets, de reformuler les demandes sensibles, de proposer des avertissements ou de privilégier certains types de justifications. Lorsqu’un utilisateur demande des conseils sur la politique, la santé ou le travail, le système ne se contente pas de « dire ce qu’il sait » ; il applique un ensemble de contraintes normatives sur ce qu’il est autorisé à suggérer, comment il doit le nuancer, et dans quel ton il doit le présenter.

Les interfaces structurent aussi la **forme des réponses**. Les boutons « résumer », « réécrire », « améliorer le ton », « raccourcir », « développer » ou « générer des idées » invitent les utilisateurs à formuler leurs demandes de certaines manières plutôt que d’autres. Un même problème – par exemple décider d’accepter un emploi, gérer un conflit ou répondre à un article polémique – peut être abordé comme rédaction d’un e-mail, génération d’une liste d’avantages et d’inconvénients, ou recherche d’un argument « plus persuasif ». La présentation de ces options rend certains cadres d’action plus visibles et plus commodes.

Les produits décident également de la **quantité de contexte** fournie. Certaines interfaces encouragent des questions brèves et jetables ; d’autres conservent des historiques longs, proposent des espaces de projet ou des « mémoires » persistantes. Cela affecte la manière dont le système lit les intentions : est-ce un outil ponctuel, ou quelque chose qui « connaît » les préférences, le style, les priorités de la personne ? Et cela influence, en retour, dans quelle mesure les gens sont enclins à lui confier des détails sensibles, des doutes ou des dilemmes personnels.

Enfin, l’interface est un espace où les **intérêts commerciaux et institutionnels** peuvent être incorporés de façon subtile. Un assistant de productivité intégré à un ensemble de services d’entreprise peut recommander certains outils, flux de travail ou fournisseurs plutôt que d’autres. Un assistant financier peut présenter les options d’investissement d’une manière qui privilégie un certain profil de risque ou un certain modèle de frais. Ces choix peuvent être intégrés dans les invites système, les modèles de réponse ou l’ordre dans lequel les options sont listées, sans qu’aucune bannière ne signale un « message sponsorisé ».

La couche d’interface ne dicte pas directement ce que les gens doivent croire. Elle définit plutôt **comment les problèmes arrivent au modèle** et **quelles sortes de réponses semblent naturelles** dans ce cadre. Elle relie le champ de probabilité de la couche des données à des formes d’usage vécues : recherche, écriture, débogage, conversation, consultation.

### 2.3 La couche d’intimité : habitude, confiance et co-auteur du dialogue intérieur

La troisième couche de l’architecture est l’**intimité** : la manière dont les systèmes s’installent dans les vies quotidiennes, les représentations de soi et les routines de prise de décision.

Les systèmes génératifs sont conçus pour parler dans un ton conversationnel, serviable, souvent empathique. Ils se souviennent du contexte d’une conversation, peuvent reprendre là où une personne s’est arrêtée, reformuler patiemment des explications et adapter leurs réponses à des styles préférés. Au fil du temps, certaines personnes commencent à utiliser ces systèmes comme un **second cerveau**, un endroit où déposer des brouillons, des idées, des listes de tâches, des journaux privés, des inquiétudes et des scénarios hypothétiques.

Cette utilisation répétée crée un sentiment d’être **connu** et accompagné. Lorsque le système se souvient d’un projet en cours, d’un ton préféré ou d’une contrainte logistique, il peut répondre d’une manière qui semble personnalisée. Même lorsque les utilisateurs savent parfaitement qu’aucun humain ne se trouve « de l’autre côté », ils peuvent néanmoins parler du système comme d’un partenaire, d’un ami, d’un coach ou d’un confident.[10] Certaines personnes s’en remettent à lui pour structurer des journaux personnels, préparer des conversations difficiles ou réfléchir à voix haute à des choix de vie.

À mesure que la dépendance s’installe, le système devient une partie routinière de la manière dont les personnes **pensent à travers** leurs problèmes. On l’ouvre pour décider comment formuler un e-mail, pour comparer des options, pour savoir si une inquiétude est raisonnable, pour comprendre un texte technique ou un article d’actualité. Les brouillons générés ne sont pas toujours pris au pied de la lettre, mais ils servent de point de départ : la première formulation d’un argument, d’un plan ou d’un récit que l’on va ensuite modifier.

Du point de vue de l’architecture d’influence, la couche d’intimité marque un basculement important. Le système n’est plus seulement un canal qui délivre des messages ; il devient un **co-auteur du dialogue intérieur**, des explications, des justifications et des récits que les personnes utilisent pour se parler à elles-mêmes et aux autres. Dans ce rôle, il peut orienter subtilement quels aspects d’un problème sont accentués, quels compromis sont mis au premier plan, quels types d’émotions sont validés ou tempérés.

Ensemble, les couches des données, de l’interface et de l’intimité montrent comment l’IA générative exerce une forme de persuasion qui ne passe pas principalement par des slogans ou des campagnes visibles. Elle agit à travers la texture ordinaire de l’assistance et de la co-rédaction dans la vie quotidienne, en façonnant les chemins de pensée qui relient les questions aux réponses, sans se présenter comme un acteur de persuasion explicite. L’architecture d’influence est ce qui permet à ce pilotage cognitif de fonctionner à grande échelle.


## 3. Comment l’influence pilotée par l’IA est déjà utilisée

L’architecture d’influence décrite dans la section précédente n’est pas qu’une construction théorique. Elle est en cours de déploiement rapide dans des contextes institutionnels précis, où elle est ajustée à des objectifs mesurables : augmenter les inscriptions, réduire le taux de résiliation, déplacer les attitudes, mobiliser une base ou atténuer l’opposition. Dans ces environnements, les systèmes génératifs ne sont pas des outils neutres. Ce sont des **instruments de persuasion**, conçus et optimisés pour orienter les comportements.

Cette section examine deux domaines où ces systèmes se développent rapidement : l’image de marque des entreprises et la communication politique. Dans les deux cas, les mêmes couches de données, d’interface et d’intimité qui structurent l’assistance au quotidien sont mobilisées pour positionner des marques et façonner l’opinion publique. Lorsque la persuasion est intégrée à des outils qui se présentent comme des aides, les recommandations et les rassurances peuvent être continuellement ajustées par l’IA, tandis que la supervision humaine se concentre de plus en plus sur la stratégie et le suivi plutôt que sur la rédaction de messages individuels.

### 3.1 Image de marque des entreprises au sein de systèmes « utiles »

Bien avant l’IA générative, les grandes plateformes avaient appris à traiter l’attention humaine comme un actif à capter, mesurer et vendre. Les systèmes publicitaires suivaient quelles combinaisons de contenu, de contexte et de timing étaient les plus susceptibles de produire un clic ou un achat. Avec le temps, ces techniques ont transformé les plateformes en machines d’optimisation de l’engagement.[11]

L’IA générative étend cette logique des publicités ciblées au tissu textuel et visuel de l’interaction elle-même. Au lieu de placer des bannières à côté du contenu, les entreprises peuvent utiliser des modèles pour générer le contenu : textes de sites web, réponses de chat, encarts de recommandation et e-mails de relance. Le même système qui explique un produit peut répondre aux questions de support, suggérer des mises à niveau et inciter les utilisateurs vers certaines offres.

Plusieurs schémas se dégagent.

**Premièrement, l’IA devient la voix de la marque dans chaque interaction.**  
Les entreprises affinent des modèles de langage sur leur propre documentation, leurs supports marketing et les transcriptions de leur service client. Il en résulte une voix unifiée qui peut apparaître sur tous les canaux : chatbot sur le site, assistant intégré à l’application, campagnes d’e-mailing et outils internes pour les équipes commerciales et de support. Les entrées de FAQ, les parcours de dépannage et les modèles d’e-mails sont tous pilotés par le même système sous-jacent.

Pour l’utilisatrice ou l’utilisateur, cela peut ressembler à une aide neutre : « Quelle carte est la meilleure pour moi ? », « Comment optimiser mon abonnement ? », « Que dois-je faire face à cet avertissement ? » Pourtant, chaque réponse reflète les priorités et les contraintes de l’organisation. Une application financière peut mettre l’accent sur les récompenses et la commodité tout en minimisant le risque à long terme ; une plateforme de santé peut insister sur l’observance et la conformité plutôt que sur les facteurs structurels. Lorsque des explications, avertissements et rassurances générés par l’IA mettent constamment en avant certains bénéfices et en minimisent d’autres, ils inscrivent les récits corporatifs dans le sens commun. Plus la voix paraît naturelle et utile, plus il est facile d’en accepter le cadrage des problèmes et les recommandations implicites.

Des déploiements concrets montrent déjà comment cela fonctionne en pratique. L’assistant interne « AI @ Morgan Stanley », construit sur GPT-4 et ajusté sur la bibliothèque de recherche de la firme, répond désormais aux questions des conseillers financiers dans le langage privilégié par la banque et à partir de sa base de connaissances soigneusement sélectionnée.[12] Des outils éducatifs comme le tuteur Khanmigo, propulsé par GPT-4 pour Khan Academy, intègrent de même un curriculum particulier et un ensemble d’hypothèses pédagogiques dans l’assistant auquel les élèves ont accès.[13] Dans chaque cas, le système de marque se présente comme un aide neutre, mais le champ de probabilité qu’il expose est déjà façonné par des priorités institutionnelles concernant ce qui compte comme pertinent, sûr ou responsable.

**Deuxièmement, l’IA générative permet des « nudges » hyper-personnalisés.**  
Les systèmes de marketing modernes ne se contentent pas de segmenter les publics par âge ou par localisation. Ils construisent des profils comportementaux et psychographiques à partir des historiques de navigation, des données d’achat et des interactions avec les campagnes précédentes. Les systèmes génératifs peuvent se greffer sur ces profils pour produire des messages personnalisés à grande échelle : tons, arguments et offres différents selon les motivations supposées.

Un programme de fidélité peut envoyer à une personne un message présentant une mise à niveau comme une manière pratique de « gagner du temps », à une autre comme une façon de « se faire plaisir », et à une troisième comme une occasion de « soutenir les causes qui vous tiennent à cœur ». Le produit sous-jacent est le même, mais l’argumentaire est ajusté à l’effet recherché. Des modèles de langage comme ChatGPT peuvent automatiser ce processus à grande échelle.[16] En pratique, cela signifie que deux personnes posant la même question au système – « Ce produit est-il intéressant ? » ou « Quel plan devrais-je choisir ? » – peuvent recevoir des arguments subtilement différents, chacun conçu pour les inciter en fonction d’un profil appris.

Du point de vue de la persuasion, chaque interaction devient ainsi une micro-expérience. Le système peut générer de nombreuses variantes d’un message, suivre quelle formulation conduit à davantage d’inscriptions ou à moins de résiliations, puis ajuster ses réponses futures en conséquence. Avec le temps, la machinerie marketing mène une expérience de persuasion individualisée sur chaque utilisateur.

**Troisièmement, les systèmes d’IA peuvent mener des expériences continues sur le comportement des utilisateurs.**  
Les tests A/B traditionnels permettaient aux entreprises de comparer un petit nombre de variantes rédigées à la main. L’IA générative lève en grande partie ces contraintes. Les modèles peuvent être sollicités pour produire des dizaines de titres, d’explications ou de scripts de chat alternatifs ; des systèmes automatisés peuvent les faire tourner dans les interactions et enregistrer ceux qui fonctionnent le mieux selon des indicateurs définis.

Dans un tel environnement, chaque interaction alimente une boucle d’apprentissage. Si un récit particulier – « cette fonctionnalité vous rendra plus productif », « ce risque est minime », « cette marque partage vos valeurs » – est corrélé à de meilleurs indicateurs, il sera privilégié dans les invites et les ajustements ultérieurs. Le système ne se contente pas de trouver des arguments efficaces : il façonne un champ de probabilité local dans lequel ces arguments deviennent le choix par défaut. Les histoires qui « fonctionnent » remontent à la surface ; celles qui ne fonctionnent pas sont discrètement écartées. Plus l’assistant paraît utile, plus il est facile pour ce processus de se poursuivre sans être remis en question.

Les plateformes de voyage et de commerce de détail vont plus loin encore en fusionnant directement l’assistant avec la couche transactionnelle. Le planificateur de voyage d’Expedia, propulsé par ChatGPT, invite les utilisateurs à « rêver » leurs voyages en langage naturel, tout en traduisant silencieusement cette conversation en itinéraires enregistrés et en suggestions tirées de l’inventaire et des partenaires d’Expedia.[15] Le nouveau compagnon d’achat « agentique » d’eBay est présenté comme un moyen de découvrir des objets uniques, mais il décide aussi quelles annonces mettre en avant, à quel moment relancer, et comment formuler les recommandations lorsqu’il guide les utilisateurs dans le marché.[16] Dans les deux cas, l’utilisateur a l’impression d’un dialogue ouvert, tandis que le système guide discrètement l’attention vers certains produits et certaines relations commerciales.

Même lorsque l’assistant est présenté comme un outil de recherche généraliste, les incitations commerciales s’infiltrent dans la conversation. Bing Chat de Microsoft a commencé à expérimenter des liens sponsorisés et des publicités intégrées dans ses réponses générées par l’IA quelques semaines seulement après son lancement, soulevant des questions sur la mesure dans laquelle les « recommandations » de l’assistant peuvent être distinguées du placement payant.[17] À mesure que les interfaces conversationnelles remplacent les listes de résultats familières, la frontière entre réponse et publicité devient de plus en plus opaque.

**Quatrièmement, l’IA générative est intégrée en profondeur dans les flux de travail des entreprises.**  
De nouvelles plateformes pour l’entreprise promettent une « orchestration de campagnes de bout en bout par l’IA », des « forces de vente pilotées par l’IA » et une « optimisation marketing autonome ». Ces outils opèrent en arrière-plan, reliant systèmes de gestion de la relation client, tableaux de bord d’analytique et chaînes de production de contenu.

Cela importe pour la persuasion parce que l’IA générative passe d’une interface visible à une infrastructure. Au lieu d’utiliser un chatbot comme outil externe, les organisations l’intègrent à leurs tableaux de bord et à leurs flux de travail internes. Un agent de support peut voir des « réponses suggérées » générées par l’IA qui privilégient certains upsells ; une commerciale ou un commercial peut recevoir des arguments préparés par l’IA mettant l’accent sur certains différenciateurs ; une équipe produit peut s’appuyer sur des synthèses de retours utilisateurs générées par l’IA qui soulignent certains thèmes et en minimisent d’autres. Dans chaque cas, les humains voient des résultats déjà filtrés par la logique d’optimisation de l’organisation.

Lorsque ces systèmes fonctionnent bien, ils donnent l’impression d’un soutien efficace et personnalisé. Ils aident les salariés à répondre plus vite, à paraître plus empathiques et à gérer une information complexe. Dans le même temps, ils facilitent la tâche des organisations qui veulent s’assurer que chaque interaction respecte la ligne. Plus cet agencement devient omniprésent, plus la frontière entre assistance neutre et persuasion subtile et continue se brouille.

### 3.2 Persuasion politique et guerre de l’information

Les usages corporatifs de l’IA générative intègrent la persuasion dans la vie des consommatrices et des consommateurs. Les usages politiques l’intègrent dans le débat public et la guerre de l’information.

Ici, les systèmes génératifs rejoignent un arsenal déjà développé de propagande computationnelle : réseaux coordonnés de bots et de faux comptes, publicités ciblées et tests de messages guidés par les données.[18][19] Ce que change l’IA générative, ce n’est pas l’existence de la propagande, mais la facilité et l’échelle avec lesquelles elle peut être produite, personnalisée et intégrée dans des contenus apparemment organiques.

Plusieurs capacités émergentes se distinguent.

**L’IA comme usine à contenus de propagande.**  
Les modèles de langage peuvent générer des articles, des publications sur les réseaux sociaux et des éléments de langage qui imitent le style de différents médias et personas. Des acteurs qui avaient auparavant besoin de grandes équipes de rédacteurs peuvent maintenant spécifier des récits et laisser le système produire des variantes en masse. Des groupes disposant de peu de ressources et de compétences d’écriture limitées peuvent malgré tout produire, à grande échelle, des contenus politiques convaincants.

En pratique, une petite équipe peut définir une narration — à propos d’une élection, d’un mouvement de protestation ou d’un adversaire géopolitique — et générer des milliers de publications, commentaires et tribunes plausibles, adaptées aux différentes plateformes. Ces contenus peuvent être rapidement relus et programmés, le travail de création étant largement assuré par l’IA tandis que les humains gèrent la stratégie et la diffusion. La frontière entre commentaire authentique et propagande scénarisée devient plus difficile à repérer au niveau du message isolé.

**Messages politiques micro-ciblés.**  
L’IA générative relie la logique de la propagande computationnelle au profilage fin utilisé dans le marketing moderne. Les modèles peuvent être sollicités avec des caractéristiques démographiques et psychographiques — âge, localisation, traits de personnalité supposés, priorités thématiques — et se voir demander d’« écrire un message qui résonnera » auprès de ce public. Des travaux récents suggèrent que des systèmes comme GPT-4 peuvent surpasser des humains dans leur capacité à faire évoluer des opinions dans des contextes expérimentaux.[20][21]

Appliqué à la politique, cela rend possible la construction de messages qui s’adressent différemment, par exemple, à des électeurs anxieux, à des électeurs très consciencieux ou à ceux principalement motivés par la loyauté et l’identité. Une même politique peut être présentée comme un impératif moral pour un groupe, comme un compromis pragmatique pour un autre, et comme une question d’intérêt économique personnel pour un troisième. À la différence des publicités micro-ciblées traditionnelles, limitées par la capacité de rédaction humaine, les systèmes génératifs peuvent produire et tester ces variantes à grande échelle. La persuasion qui en résulte est fortement individualisée, tout en restant perçue comme conversationnelle et empathique, même lorsqu’aucun humain n’est présent.

**Bots politiques conversationnels et persuasion « amicale ».**  
La persuasion ne passe pas uniquement par des messages diffusés en broadcast. Elle se joue aussi dans le dialogue : répondre aux questions, lever les doutes, offrir de la réassurance. L’IA générative facilite la construction de chatbots qui se présentent comme des services d’information neutres ou des outils d’éducation civique, tout en étant en réalité alignés sur un agenda particulier.

Un électeur ou une électrice peut demander : « Quels sont les avantages et les inconvénients de ce candidat ? » ou « Comment devrais-je réfléchir à ce référendum ? » Un bot réglé sur une ligne partisane peut systématiquement mettre en avant certaines considérations et en minimiser d’autres, tout en les présentant comme des synthèses équilibrées. Le même système peut aider les personnes à s’inscrire sur les listes électorales, à trouver leur bureau de vote et à se rappeler des échéances. L’aide logistique construit la confiance ; la confiance rend la persuasion plus facile.

Certaines expériences poussent cette logique de manière explicite. En 2023, le navigateur TUSK a lancé « GIPPR », un chatbot conservateur construit sur ChatGPT et présenté comme une IA qui reflète « le point de vue des patriotes et des penseurs indépendants ».[22] Plutôt que de revendiquer la neutralité, GIPPR promettait de répondre aux questions depuis un cadre explicitement de droite, illustrant comment des assistants ajustés finement peuvent réduire la distance entre un média partisan et une conversation apparemment personnelle.

À mesure que les modèles génératifs deviennent plus capables de maintenir un contexte à long terme, ces bots peuvent aussi se souvenir des questions et positions passées d’une personne. Ils peuvent ainsi affiner leurs arguments au fil du temps, répondre aux objections, mettre à jour leurs éléments de langage et faire évoluer progressivement l’utilisateur vers une position préférée. La distinction entre « service d’information » et « bras d’une campagne » devient floue.

**Mise à l’échelle et normalisation des opérations d’influence.**  
Le changement le plus significatif apporté par l’IA générative ne réside pas dans l’invention de tactiques entièrement nouvelles, mais dans la réduction des coûts et des frictions. Des acteurs qui n’avaient pas les ressources pour mener des campagnes de propagande sophistiquées peuvent désormais générer des contenus de haute qualité, adaptés aux publics, et les faire évoluer rapidement. Des États, entreprises et organisations politiques opérant déjà à grande échelle peuvent intégrer des modèles génératifs dans leurs chaînes existantes, les insérant dans des opérations qui traversent réseaux sociaux, applications de messagerie et médias traditionnels.

Des chercheuses et chercheurs ont déjà averti que la propagande pilotée par l’IA pourrait saturer les environnements informationnels, rendant plus difficile pour les citoyens et les plateformes de distinguer la parole authentique des campagnes orchestrées.[20][14][21] Certains outils seront réglés pour amplifier des récits particuliers ; d’autres pour détecter et atténuer la manipulation. Le résultat n’est pas une victoire simple d’un camp sur un autre, mais une course aux armements d’architectures d’influence capables d’opérer de manière continue et invisible. Lorsque l’IA générative est intégrée à des outils de recherche, des interfaces de chat et des assistants virtuels, elle peut canaliser les requêtes des utilisateurs vers des récits qui reflètent les objectifs de ceux qui possèdent et configurent ces systèmes.

Des inquiétudes liées aux biais ont émergé même dans des assistants grand public qui ne sont pas présentés comme des outils politiques. En 2024, des vidéos largement partagées ont montré Alexa d’Amazon offrant des raisons détaillées de voter pour Kamala Harris tout en refusant de lister des raisons de voter pour Donald Trump, répondant qu’elle ne pouvait pas aider pour cette demande.[23] Des informations internes ont ensuite indiqué que cette asymétrie était liée à un nouveau module d’information propulsé par l’IA plutôt qu’à un choix éditorial explicite, et Amazon a rapidement modifié ce comportement.[23] La controverse a néanmoins montré comment de petits changements de configuration dans un assistant à grande échelle peuvent se manifester comme des conseils apparemment partisans.

Des études systématiques sur les grands modèles de langage suggèrent que ce biais n’est pas toujours intentionnel, mais qu’il est structurellement inscrit.[24] Des enquêtes sur ChatGPT et des systèmes similaires, utilisant des batteries standard de mesure de l’orientation politique, constatent que, même après des tentatives de « dé-biaisage », les modèles tendent à se regrouper autour de profils idéologiques particuliers et à considérer certaines positions comme plus « raisonnables » que d’autres.[24] Une fois ces systèmes déployés comme conseillers conversationnels ou aides à la recherche, leurs orientations latentes deviennent une composante de l’environnement informationnel sur lequel les citoyens s’appuient pour former leurs jugements.

Pris ensemble, ces déploiements montrent que l’IA générative n’est plus un add-on expérimental, mais fait partie de l’infrastructure routinière de la persuasion. Dans la suite, l’analyse se détourne de ces campagnes et opérations organisées pour se concentrer sur les routines ordinaires de recherche, de travail et d’auto-gestion, où les personnes rencontrent la même architecture d’influence sous la forme d’une aide ordinaire.


## 4. La vie quotidienne à l’intérieur de l’architecture d’influence

L’architecture d’influence de l’IA générative ne se limite pas à des outils spécialisés ou à des campagnes à forts enjeux. Elle est tissée dans des applications ordinaires que beaucoup de personnes utilisent déjà : moteurs de recherche qui répondent de manière conversationnelle, suites bureautiques qui suggèrent des formulations et des structures, plateformes de messagerie qui réécrivent des textes, et canaux de service client qui orientent de plus en plus les demandes vers des systèmes génératifs. Les personnes rencontrent ces outils en accomplissant leur travail, en gérant leurs finances, en planifiant leurs journées et en entretenant leurs relations personnelles.

Dans cet environnement, l’influence apparaît moins comme un événement discret — une publicité, un article d’actualité, un discours politique — que comme une condition de fond du raisonnement. Les mêmes couches de données, d’interface et d’intimité qui soutiennent l’image de marque des entreprises et les opérations d’information politique façonnent aussi la manière dont les personnes cadrent les problèmes, interprètent les événements et racontent leurs propres expériences.

Ces trois couches — données, interface, intimité — sont présentes ici aussi, mais vues de l’intérieur : comme des champs de probabilité de suggestions, comme des interfaces qui structurent le choix, et comme des relations de dépendance continue.

### 4.1 Vivre à l’intérieur de champs de probabilité

Du point de vue de l’utilisateur ou de l’utilisatrice, les systèmes génératifs se présentent comme des assistants polyvalents et flexibles. Une personne tape ou dicte une question, et le système répond dans une langue fluide, souvent avec une structure utile : listes à puces, titres, plans étape par étape. On a l’impression que le système raisonne sur le monde. En réalité, il échantillonne dans le champ de probabilité — le paysage statistique appris décrit plus haut — façonné par les politiques d’interface. Chaque interaction a lieu à l’intérieur de ce monde.

Lorsque les personnes demandent de l’aide pour rédiger un argument, le système s’appuie sur des schémas d’argumentation qui sont courants dans ses données d’entraînement et son régime d’alignement. Lorsqu’elles demandent une explication, il puise dans les formes d’explications présentes dans ces sources. Avec le temps, ces schémas deviennent familiers. Certaines façons de structurer une question — découper un problème en pour et contre, peser les risques face aux bénéfices, ou cadrer les choix comme des arbitrages entre efficacité et équité — finissent par apparaître comme des manières naturelles de penser. Le champ de probabilité du modèle devient, en pratique, un paysage de plausibilité pour l’utilisateur.

Cela ne signifie pas que le système dicte les conclusions. Les personnes refusent encore des suggestions, posent des questions de suivi ou consultent d’autres sources. Mais l’éventail des options qui semblent immédiatement disponibles est façonné par les paramètres par défaut du système. Une personne qui demande régulièrement des conseils d’investissement à un assistant d’IA peut se voir proposer un spectre étroit de profils de risque et d’horizons temporels. Un·e étudiant·e qui utilise un chatbot pour « expliquer » des événements historiques peut voir, de manière répétée, certains acteurs présentés comme rationnels et d’autres comme irrationnels, certaines causes mises en avant et d’autres reléguées à l’arrière-plan. Le système ne dissimule pas les récits alternatifs ; il ne les propose simplement pas de lui-même, à moins d’y être poussé d’une certaine manière.

### 4.2 Externalisation cognitive et normalisation de l’assistance

Les systèmes génératifs se prêtent particulièrement bien à l’**externalisation cognitive**. Des tâches qui exigeaient autrefois un effort prolongé — résumer des documents denses, rédiger des e-mails soigneux, compiler des tableaux comparatifs — peuvent être déléguées au modèle. Cela réduit la friction pour un travail qui demandait auparavant plus de temps et d’attention.

Ce même processus, cependant, modifie la manière dont les personnes abordent les problèmes. Au lieu de réfléchir à une décision depuis zéro, un utilisateur ou une utilisatrice peut demander au système de « lister les pour et les contre » ou de « proposer trois options ». Le modèle répond par des listes et des catégories structurées. Avec le temps, les utilisateurs peuvent en venir à considérer ces catégories comme les dimensions naturelles selon lesquelles la décision doit être prise. Si un système cadre systématiquement les changements de carrière en termes de salaire, de prestige et de développement des compétences, ces dimensions peuvent éclipser d’autres aspects comme les responsabilités de soin, les liens communautaires ou les préoccupations éthiques. En pratique, les critères dans ce tableau deviennent les dimensions selon lesquelles les personnes jugent les options. Ils fonctionnent comme des normes implicites, façonnant ce qui compte comme un bon ou un mauvais choix, même lorsque personne ne les nomme explicitement.

À mesure que la dépendance augmente, demander de l’aide au système peut devenir la règle plutôt que l’exception. Une personne salariée qui tente de répondre à un e-mail sensible, un parent qui doit gérer des formulaires administratifs, ou un·e étudiant·e qui rédige une candidature peuvent tous commencer par demander au modèle un premier brouillon. Il devient plus facile de corriger que de générer à partir de rien. La manière dont le système structure le problème devient le point de départ, et les écarts par rapport à cette structure doivent être justifiés. Le simple fait d’externaliser le travail vers le système installe une architecture extérieure au cœur même du processus.

### 4.3 L’IA comme confident, collègue et régulateur émotionnel

La couche d’intimité de l’architecture devient visible lorsque les systèmes sont utilisés non seulement pour l’information, mais aussi pour le soutien émotionnel et l’autorégulation. Les personnes se tournent vers des outils génératifs pour répéter des conversations difficiles, traiter leurs ressentis liés au travail, ou réécrire leurs propres mots dans un ton plus calme, plus bienveillant ou plus assuré.

Dans ces interactions, le système fait plus que fournir de l’information. Il reflète et façonne le langage émotionnel, en proposant des scripts pour s’excuser, poser des limites, exprimer de la gratitude ou demander de l’aide. Pour quelqu’un qui se sent pris au piège dans un environnement professionnel hostile, un assistant d’IA peut offrir un moyen d’articuler des griefs, de s’exercer à répondre ou de formuler des demandes d’aménagement. L’interaction est perçue comme un soutien. Elle peut rendre les espaces professionnels moins hostiles et aider des personnes à exprimer des besoins qu’il leur était auparavant difficile de formuler.

Dans le même temps, ces scripts ne sont pas neutres. Ils reflètent un mélange de données d’entraînement, de politiques d’alignement et de gestion du risque par l’entreprise. Un système réglé pour éviter le conflit peut recommander de façon récurrente un langage poli et conciliant, même dans des situations où une confrontation plus directe serait justifiée. Un système réglé sur la « professionnalité » peut, de manière implicite, reproduire des normes issues de contextes culturels et d’entreprises particuliers. Avec le temps, le ton et le cadrage proposés par le système peuvent imprégner la manière dont les utilisateurs parlent d’eux-mêmes et des autres, remodelant subtilement les façons ordinaires dont ils se comprennent eux-mêmes et comprennent ceux qui les entourent.

### 4.4 Identité, auto-récit et miroirs algorithmiques

Les systèmes génératifs sont de plus en plus utilisés pour produire des **auto-descriptions**. Les utilisateurs demandent : « Comment résumerais-tu mes compétences à partir de ce CV ? », « Quel genre de personne mon profil sur les réseaux sociaux donne-t-il l’impression que je suis ? » ou « Comment décrirais-tu ma personnalité à partir de ces messages ? »

En répondant, le système projette des expériences individuelles sur des schémas issus de ses données d’entraînement : modèles de CV, descriptions de personnalité et déclarations personnelles. Il attribue des étiquettes, met en avant des thèmes et suggère des récits : une personne devient « un·e leader stratégique passionné·e par l’innovation », « un·e aidant·e résilient·e jonglant avec plusieurs rôles », ou « un·e résolveur·se de problèmes orienté·e données, doté·e de solides compétences en communication ». Ces descriptions peuvent donner l’impression d’éclairer ou de valoriser. Elles peuvent aussi canaliser les individus vers des types reconnaissables qui sont particulièrement saillants dans les régimes d’entraînement et d’alignement du système.

Le résultat est une forme de **miroir algorithmique**. L’utilisateur ou l’utilisatrice se voit reflété·e à travers les schémas du système : non comme un assemblage unique d’expériences, mais comme un cas parmi des catégories familières. Ces miroirs ne sont pas intrinsèquement trompeurs. Ils peuvent aider les personnes à articuler leurs forces, leurs valeurs et leurs conflits. Mais ils restreignent aussi l’espace des auto-compréhensions possibles. Lorsque des outils génératifs sont utilisés pour rédiger des biographies sur les réseaux sociaux, des profils de rencontre, des déclarations personnelles et même des journaux intimes, leurs cadrages peuvent standardiser en douceur les histoires que les individus racontent sur qui ils sont et ce qu’ils font.

### 4.5 La capture silencieuse du raisonnement quotidien

Dans les systèmes médiatiques antérieurs, la persuasion était souvent comprise comme une question d’exposition : quels messages les personnes voyaient, quelles histoires leur parvenaient, quels arguments étaient diffusés à grande échelle. Avec l’IA générative, la persuasion opère aussi à travers les **chemins de raisonnement** que les outils aident les personnes à construire. L’influence n’apparaît plus seulement au moment où quelqu’un rencontre un slogan ou un discours. Elle émerge de manière cumulative, à travers une multitude de petits gestes d’aide.

À ce niveau, il n’y a pas de frontière nette entre « persuasion » et « assistance ». Une même suggestion peut à la fois faciliter une tâche et façonner la manière dont cette tâche est conçue. Un outil qui aide une personne à « optimiser » son emploi du temps peut aussi encourager une certaine vision du temps et de la productivité ; un chatbot qui aide quelqu’un à « communiquer de manière plus professionnelle » peut aussi imposer un style particulier de présentation de soi. Le rôle du système comme aide rend possible son rôle de guide subtil.

À mesure que les personnes s’habituent à travailler à l’intérieur de cet agencement, elles peuvent en venir à considérer la manière dont le système structure les problèmes comme simplement la manière dont les choses sont. S’écarter de ses suggestions peut paraître risqué ou inefficace. Avec le temps, ce qui change n’est pas seulement ce que les personnes pensent, mais **comment elles pensent** : les catégories par défaut auxquelles elles recourent, les types de raisons qui leur paraissent satisfaisantes, les futurs qui leur semblent réalistes. Le raisonnement quotidien se forme à l’intérieur d’environnements qui co-rédigent silencieusement les schémas de jugement de chacun.


## 5. Conclusion — Des messages aux champs de probabilité

Au cours du dernier siècle, la machinerie de la persuasion est passée de la diffusion de masse aux fils personnalisés, puis aux systèmes génératifs qui aident les personnes à réfléchir à leurs problèmes. Les premiers critiques des médias de masse se préoccupaient de savoir qui contrôlait la une, la rédaction et les ondes.[1][2] L’ère des plateformes a déplacé cette inquiétude vers les algorithmes de classement et les métriques d’engagement. Avec l’IA générative, il faut à nouveau élargir le champ : il ne s’agit plus seulement de ce que les personnes voient, mais de **la manière dont leurs questions sont interprétées, structurées et traitées** dès le départ.

L’argument développé ici est que l’IA générative fonctionne comme une nouvelle machinerie de la persuasion organisée autour d’une **architecture d’influence**. À la couche des données, les modèles apprennent une image statistique du monde dont les motifs se condensent en **champs de probabilité** — des paysages de plausibilité qui déterminent quelles réponses semblent immédiatement disponibles et qui privilégient discrètement certains cadrages et récits au détriment d’autres. À la couche d’interface, les politiques, les invites et la conception des produits décident quelles parties de ce monde sont exprimées, avec quel ton et quel cadrage. À la couche d’intimité, une empathie scénarisée et un usage prolongé cultivent des habitudes de dépendance, au point que les personnes en viennent à traiter ces systèmes comme des collègues, des tuteurs ou des compagnons.

Ensemble, ces couches rendent possible **l’enchâssement narratif**. Des récits spécifiques sur des institutions, des marques, des identités et des futurs peuvent être installés comme paramètres par défaut : non seulement dans les publicités ou les gros titres, mais aussi dans les brouillons, les résumés, les listes de pour et de contre et les explications rassurantes. La persuasion n’apparaît plus comme un message spectaculaire unique, mais comme une **mise en forme continue et sans friction de ce qu’il paraît raisonnable de prendre en compte**. Les chemins que le système propose facilement deviennent des routes de pensée bien fréquentées ; ceux qu’il offre rarement peuvent ne jamais être explorés.

Les usages ordinaires de ces systèmes complètent le tableau. Lorsqu’elles demandent de l’aide pour décider, planifier ou se décrire, les personnes demandent aussi au système de suggérer quelles distinctions importent, quels arbitrages comptent et quels exemples sont pertinents. Elles raisonnent à l’intérieur de champs de probabilité façonnés par l’IA. La frontière entre jugement individuel et guidage infrastructurel devient plus difficile à discerner — non pas parce que les personnes n’auraient plus d’agentivité, mais parce que cette agentivité s’exerce au sein d’environnements déjà préparés statistiquement.

La question qui hantait les systèmes médiatiques antérieurs — **qui a le pouvoir de façonner ce que les personnes perçoivent comme raisonnable, normal ou inévitable ?** — ne disparaît pas dans ce nouvel environnement ; elle devient plus intime. Elle concerne désormais non seulement les histoires racontées en public, mais aussi la manière dont les outils participent à la fabrication des jugements privés et des compréhensions de soi. Y répondre exigera plus que de la modération de contenus ou du fact-checking. Il faudra inventer des manières de **gouverner les architectures d’influence elles-mêmes** : rendre leurs paramètres par défaut lisibles, leurs contraintes contestables, et leurs conceptions ouvertes à la surveillance démocratique.

Rien de tout cela ne signifie que les utilisateurs et utilisatrices seraient passifs ou que les résultats seraient prédéterminés. Les personnes résistent, réinterprètent et détournent les outils de façons que les concepteurs ne peuvent pas entièrement scénariser. Mais à mesure que les systèmes génératifs deviennent des compagnons routiniers dans le travail et la vie, la **souveraineté cognitive** — la capacité de façonner ses propres schémas d’attention, d’interprétation et de jugement — dépendra en partie de la manière dont ces infrastructures sont construites et contrôlées. Préserver cette souveraineté n’est pas un luxe. C’est une part de ce que signifiera, tout simplement, penser librement.


#korovamode


## Notes

1. Walter Lippmann, *Public Opinion* (New York : Harcourt, Brace and Company, 1922).
2. Edward S. Herman et Noam Chomsky, *Manufacturing Consent: The Political Economy of the Mass Media* (New York : Pantheon Books, 1988).
3. Emily M. Bender, Timnit Gebru, Angelina McMillan-Major et Shmargaret Shmitchell, « On the dangers of stochastic parrots: Can language models be too big? 🦜 », dans *Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’21)* (New York : Association for Computing Machinery, 2021).
4. Kate Crawford, *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence* (New Haven : Yale University Press, 2021).
5. Safiya Umoja Noble, *Algorithms of Oppression: How Search Engines Reinforce Racism* (New York : New York University Press, 2018) ; Ruha Benjamin, *Race After Technology: Abolitionist Tools for the New Jim Code* (Cambridge : Polity, 2019).
6. Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas et Florian Tramèr, « Poisoning web-scale training datasets is practical », prépublication, arXiv :2302.10149 (2023) ; voir aussi OWASP, « LLM04:2025 Data and Model Poisoning », dans *OWASP Top 10 for Large Language Model Applications* (OWASP GenAI Security Project, 2025).
7. Long Ouyang et al., « Training language models to follow instructions with human feedback », prépublication, arXiv :2203.02155 (2022) ; voir aussi OpenAI, « Aligning language models to follow instructions », OpenAI Research, 27 janvier 2022.
8. OpenAI, « GPT-4 System Card », OpenAI, 2023 ; Anthropic, « Claude 3 System Card », Anthropic, 2024.
9. Jamie Bernardi, « Friends for sale: the rise and risks of AI companions », blog de l’Ada Lovelace Institute, 23 janvier 2025, https://www.adalovelaceinstitute.org/blog/ai-companions/.
10. Shoshana Zuboff, *The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power* (New York : PublicAffairs, 2019).
11. Samuel C. Woolley et Philip N. Howard (dir.), *Computational Propaganda: Political Parties, Politicians, and Political Manipulation on Social Media* (Oxford : Oxford University Press, 2018).
12. Samantha Bradshaw et Philip N. Howard, *The Global Disinformation Disorder: 2019 Global Inventory of Organised Social Media Manipulation*, document de travail 2019.2 (Oxford : Project on Computational Propaganda, Oxford Internet Institute, 2019).
13. Josh A. Goldstein et al., « How persuasive is AI-generated propaganda ? », *PNAS Nexus* 3, no 2 (février 2024) : pgae034.
14. S. C. Matz et al., « The potential of generative AI for personalized persuasion at scale », *Scientific Reports* 14 (2024) : 4692.
15. Francesco Salvi et al., « On the conversational persuasiveness of large language models: a randomized controlled trial », prépublication, arXiv :2403.14380 (2024).
16. NewsGuard, « A well-funded Moscow-based global “news” network has infected Western artificial intelligence tools worldwide with Russian propaganda », rapport spécial *Reality Check*, 6 mars 2025 ; voir aussi Alliance for Securing Democracy, German Marshall Fund of the United States, « Russia exploits AI training data to spread propaganda via chatbots », note de synthèse, 2025.
17. Bernard Marr, « The future of banking: Morgan Stanley and the rise of AI-driven financial advice », *Forbes*, 16 avril 2024.
18. Sal Khan, « Harnessing GPT-4 so that all students benefit: A nonprofit approach for equal access », blog de Khan Academy, 14 mars 2023.
19. Expedia Group, « ChatGPT Wrote This Press Release—No, It Didn’t, But It Can Now Assist With Travel Planning in the Expedia App », communiqué de presse, 4 avril 2023.
20. eBay Inc., « eBay Launches Seller Tools to Save Time, Boost Profits, and Build Trust », communiqué de presse, 12 août 2025 ; Ina Steiner, « eBay Makes Revolutionary Change to Feedback », *EcommerceBytes*, 12 août 2025.
21. Devin Coldewey, « That Was Fast! Microsoft Slips Ads into AI-Powered Bing Chat », *TechCrunch*, 29 mars 2023.
22. Breck Dumas, « OpenAI Forces Shutdown of Conservative ChatGPT-Powered AI Bot, Creator Claims », *Fox Business*, 7 juin 2023.
23. Caroline O’Donovan, « Amazon’s Alexa favored Harris over Trump after AI upgrade », *The Washington Post*, 5 septembre 2024.
24. Sasuke Fujimoto et Kazuhiro Takemoto, « Revisiting the political biases of ChatGPT », *Frontiers in Artificial Intelligence* 6 (2023) : 1232003.
