# The Augmented Self
## AI Scaffolds, Offloading, and the Drift Toward Dependency

Korovamode | December 28, 2025


**Abstract**  
The Augmented Self argues that everyday AI assistants are becoming cognitive scaffolds, shaping not just final wording but early-stage interpretation, framing, and next-step selection. As this “intermediate cognition layer” becomes routine, users may shift from originator mode to editor mode—selecting and revising suggested options rather than generating first frames. The essay separates offloading (what is delegated) from mediation (how suggestions steer attention and perceived completeness), and warns that fluent output can reduce reframing and verification, producing gradual drift. Dependency becomes clearest under interruption, so the essay proposes removability and workflow resilience as practical guides to appropriate reliance.

**Keywords**  
AI assistants; cognitive scaffolding; intermediate cognition layer; cognitive offloading; originator mode; editor mode; mediation; suggested options; framing effects; automation bias; fluency-as-understanding; confidence laundering; deskilling; skill atrophy; drift; dependency; removability; resilience; appropriate reliance; trust calibration. 


## 1) Everyday AI Assistance Is Now the Baseline


AI assistants are increasingly used in everyday work for tasks that once required active deliberation. They can quickly produce workable language—drafts, tone adjustments, and suggested replies that the user edits before proceeding.[1][2] They can also reduce the effort of handling information by summarizing long text, clarifying unfamiliar terms, and turning a loose intention into a simple plan.[3][4] As this kind of first-pass support becomes routine, it shifts how work is started: from originating an initial frame to selecting and revising suggested options.

This is part of a longer story of enhancement through tools. Spelling and grammar checks reduced surface errors that used to consume attention. Templates and autocomplete reduced repetitive phrasing and formatting work. Rules, filters, and macros removed steps a user once performed manually. Each step made the work smoother, faster, and more consistent.

What is changing now is the level at which assistance operates. It no longer helps only with polish. It increasingly participates in the early moves by offering an interpretation of what a document is saying, suggesting possible implications, proposing an initial framing for a task, and generating plausible next steps.

An **intermediate cognition layer** is now available on demand. It sits between a user’s raw input and a finished output, providing a quick external pass that turns ambiguity into something workable—an outline, a draft, an action list, or a provisional framing.

This makes **cognitive offloading** easier earlier in the workflow. A user can delegate the early moves before they know what they think or how they want to proceed. The delegated output is not a final decision; it is a provisional handle that the user can accept, reject, or reshape.

In this role, the assistant supplies a **first pass**: the first workable version of the thinking and the language. It offers an initial interpretation and a starting direction, along with rough wording that gives the user something concrete to revise.

When that layer supplies the early moves, it functions as a **scaffold**—a support layer that makes work easier while it is present, and reveals its role when it is removed.[5] The user still does the work, but the support changes what “starting” feels like. It reduces the friction of getting started.

The **scaffold** does more than provide information. It produces **suggested options** for meaning and action—candidate framings, implied stances, and drafts in a chosen voice.[6] These are not answers to accept. They are options to revise.

A simple example captures the pattern. A user receives a dense or delicate message. The assistant proposes a reply in an appropriate style.[2] The user makes edits and then sends. The result can read fluent even when the earliest interpretive work has been partially externalized.[7]

For students, the same mechanism appears in essay writing: prompt restatement, outline suggestions, draft paragraphs, and revision help.[8] The student still revises, but the first workable direction and wording are often supplied by the assistant.[9]

When this layer is cheap and always available, it can become the default way to bridge small gaps in comprehension and execution. The gaps are rarely dramatic; they show up as ordinary moments of uncertainty where a user would otherwise stop and work out an initial interpretation and some starting language. Over time, the workflow can shift from generating a frame to selecting among ready-made frames.[7]



## 2) The Practical Role Shift: Originator Mode → Editor Mode

As this kind of assistance becomes routine, it changes how work is started and stabilized. The shift is that an external scaffold increasingly supplies the first coherent direction, not merely faster wording. A user’s role can move from producing a first pass to selecting and revising one.[7][8] Survey research on knowledge workers reports self-reported reductions in cognitive effort and confidence effects when using generative AI tools.[10]

In **originator mode**, a user begins with uncertainty and generates a first frame internally. The user decides what the task is and what matters in it. The user also chooses a stance to take. Then the user produces initial language that makes the work concrete enough to revise.

In **editor mode**, the first frame and first wording often arrive as **suggested options**. The tool proposes an interpretation, a plan, or a draft in an appropriate style. The user still edits. But the earliest moves—what it means and how to begin—are no longer consistently generated from scratch.[7]

Some research describes this pattern as **human–AI co-writing** and as **post-editing**. The user steers by revising. The tool supplies a first pass that the user reshapes.[8]

This shift can be hard to notice because revision remains visible. The user still revises—changing sentences, correcting details, and making tone adjustments. What becomes less visible is the starting point. The workflow begins from an externally supplied first pass that already carries a frame and can borrow the tool’s confidence about what to do next.[10][7]

There are real gains. Editor mode can reduce friction and accelerate routine writing. It can also make unfamiliar tasks more tractable by lowering the cost of reaching a workable first draft. Controlled experiments on professional writing tasks report measurable productivity improvements, with mixed but often positive effects on output quality.[11] In organizational settings, these workflows can increase consistency in routine communications and help people reach an acceptable baseline faster.[1]

But the role shift also changes what gets practiced. In originator mode, producing a first pass is a training ground for framing and formulation. It is also a training ground for self-checking while the work is still unstable. In editor mode, those capacities are used intermittently. The scaffold can supply the initial structure whenever a user hesitates.[7] Over time, a user can become more fluent at revising options than at generating first ones.[7]

This role shift is powered by **offloading** and **mediation**. Offloading delegates intermediate cognition to the scaffold. Mediation is the way the scaffold’s options shape what feels salient and safe to do next.[7]


## 3) Offloading: What Is Being Delegated

The shift into editor mode is powered by **offloading**. Offloading does not mean only storing information elsewhere or retrieving facts faster. It means delegating **intermediate cognition**. It is the first pass of interpretation, framing, formulation, and checking that turns uncertainty into a workable next step.[7]

Offloading can be hard to notice because a user remains “in the loop.” The user still decides what to keep, what to reject, and what to send. But the scaffold increasingly supplies the early structure: a provisional reading of what the text seems to say, what seems to matter, what posture to take, and language that already counts as an acceptable first draft.

What gets delegated tends to cluster in four places. First is **interpretation**: a quick account of what something means in context. Second is **formulation**: turning an intention into usable language. Third is **checking**: plausibility, completeness, and basic verification. Fourth is **next-step selection**: translating a situation into a plan, recommendation, or response. When these are supplied on demand, work often begins already stabilized.[7]

This matters because the first pass is not only a production step. It is also where key capacities are practiced. Producing an initial frame exercises the ability to hold ambiguity and decide what is salient. It also exercises the ability to generate language before anything feels settled. When offloading becomes the default response to hesitation, these capacities are practiced less often and in a thinner form.[7]

Offloading can also change memory strategy. When the scaffold reliably supplies a first pass, a user has less reason to retain intermediate steps internally. Those steps become something to retrieve on demand.[12] Offloading can also change attention. Related work suggests that the ready presence of an external aid can reduce available cognitive capacity, even when a user is not actively using it.[13] This is not automatically harmful, but it changes what a user can do quickly and confidently without the scaffold.[7]


## 4) Mediation: How the Scaffold Steers Work

Offloading describes what is delegated. **Mediation** describes what happens to the work as a result. The scaffold intervenes by supplying a pre-structured way to see the situation and act within it. It supplies an interpretation. It supplies a frame. It supplies a ready next step in fluent language.

This matters because **suggested options** are not neutral containers. Each option arrives with an implicit stance. It implies what is salient and what counts as the problem. It implies what risks matter. It implies what kind of response is appropriate. Even when a user revises, the starting point narrows attention. One framing becomes immediately workable. Other framings become less available.[6]

This is the logic of **choice architecture** and **decision support**. The option set is already a form of steering.[6][14]

Mediation is often experienced as clarity. A user moves from ambiguity to a coherent path: what it means, what seems salient, and what to do next. This can be genuinely helpful, especially under time pressure or unfamiliarity. Survey research on knowledge workers reports self-reported reductions in cognitive effort and confidence effects when using generative AI tools.[10] When coherence arrives quickly and effort drops, internal cues that normally trigger re-framing or deeper checking can become less likely to fire.[10][15][16]

The central risk is not that people will “trust AI” in a global sense. The risk is **miscalibrated reliance**. It is a form of **overreliance** in contexts where the scaffold’s outputs are not reliably appropriate. This includes treating confident prose as a proxy for correctness. It includes treating plausible summaries as verified. It includes letting a suggested plan set the default risk posture without re-framing.[15][16]

Classic analyses of automation describe a family of failure modes around this calibration problem. **Misuse** is over-reliance. **Disuse** is under-use even when the system would help. **Abuse** is design or incentives that push reliance beyond what is warranted. The details vary by domain, but the underlying issue is the same. Mediation changes how human judgment is distributed across a system.[17]

Recent work specific to language models emphasizes the same calibration problem and explores interventions. Examples include exposing uncertainty, surfacing sources, highlighting inconsistencies, and making limits legible at the point of use. These are not cosmetic features. They are attempts to reshape mediation so that “acceptable prose” does not collapse into “acceptable reasoning.”[18]


## 5) Drift: How Habits and Capacities Shift

Over time, routine scaffolding can produce **drift**. Drift is not a single failure or a dramatic collapse. It is a gradual change in what gets practiced, what becomes automatic, and what starts to feel effortful. When the first pass is frequently supplied externally, the internal capacities that would have produced it are exercised less often. They can thin accordingly. In broader discourse this is often discussed as **deskilling** or **skill atrophy**. Drift names the same process at the level of everyday workflow.[7]

Three kinds of drift are especially common: **interpretation drift**, **formulation drift**, and **verification drift**.

**Interpretation drift** is the tendency to rely on external summaries and framings rather than building an internal account. A user becomes faster at accepting or lightly revising an offered interpretation than at generating a first one from ambiguity. Over time, the “sense-making-to-start” step is more often outsourced than practiced.[7]

**Formulation drift** is the tendency to treat language as something selected and tuned rather than generated. A user becomes fluent at revision—tone, register, structure—while the ability to produce a first draft under uncertainty is practiced less often. A user may still write well, but the pathway increasingly runs through options provided by the scaffold.[7]

**Verification drift** is the tendency to check less because the output arrives fluent and complete. A user learns that the scaffold usually provides something workable. Internal triggers for skepticism weaken, especially in low-stakes, high-frequency contexts. This overlaps with **automation bias**: accepting a system’s output too readily because it presents as a finished answer.[19][17] In the case of language models, the risk is amplified by outputs that can sound fluent and convincing even when wrong.[18][16]

These drifts are amplified by familiar cognitive dynamics. One is **fluency-as-understanding**: treating smooth language as evidence that the underlying reasoning is sound. Another is **confidence laundering**: uncertainty is transformed into crisp prose that carries a completion signal. The work can feel resolved even when it has not been verified.[18]

These drifts are not symmetric. Revision can remain strong even as initiation weakens. A user can remain capable of improving language while becoming less practiced at generating first frames and performing independent checks. The result is not ignorance. It is a re-weighting of what the workflow trains.


## 6) Agency: Options, Frames, and the Subtle Narrowing of Choice

The scaffold is often described as “help,” but its influence is exercised through **framing**. It does not only accelerate execution. It proposes what the situation is and what matters in it. It also implies what kind of response counts as appropriate. Even when a user revises, the starting frame can shape what feels thinkable, urgent, and worth doing first.[6]

This is why “suggested options” is not a neutral description. Options arrive with defaults. They carry a risk posture. They carry a tone. They carry an implied priority structure. A user’s role can shift toward selection among pre-shaped paths rather than deliberation from open uncertainty. Agency can remain present, but it becomes easier to confuse acceptance-with-edits for independent judgment. One way to name this shift is **epistemic dependence**: a user’s first sense of “what this is” is increasingly supplied rather than built.[6]

The problem is not that the scaffold removes choice. The problem is that it can compress the moment when a user would ordinarily generate a frame. When the first coherent interpretation arrives immediately, alternative framings must be actively produced rather than passively discovered. Over time, “starting from options” can become the background expectation of how thinking begins.[6][7]

Agency in editor mode becomes less automatic. The first workable frame arrives already stabilized, so authorship often shifts to a narrower question: whether to adopt the offered frame, and how far to depart from it. What tends to drop out is the brief interval in which alternative framings would otherwise be generated. When that interval is missing, “starting from options” can function as a default even while revision remains visible.[6][7]

This is also where “appropriate reliance” becomes operational rather than abstract. Reliance is less a global judgment about trust and more a repeated calibration of what the scaffold is allowed to settle—interpretation, stance, risk posture, or next steps—before independent reframing occurs. Design features that surface uncertainty, expose sources, or highlight inconsistencies can support that calibration, but they do not eliminate the underlying dependence on active ownership of framing.[18][15][16]


## 7) Dependency Becomes Legible: Interruption and Constraint

Dependency is often most visible under interruption or constraint. When access changes—temporary unavailability, tiering, rate limits, policy gating, or contextual restrictions—the effects tend to appear as distributed impairments rather than dramatic collapse. Work still happens, but it becomes uneven. A user notices friction in places that used to be quietly carried by the scaffold.[7]

The impairments are typically upstream. **Starting** becomes harder because a first coherent frame is no longer supplied on demand. **Formulation** slows because initial language is no longer available as a suggested option to revise. **Checking** becomes thinner because verification no longer arrives bundled with completion signals and confident prose. **Uncertainty** rises because the workflow no longer includes a fast external pass that stabilizes meaning and next steps.[7]

Dependency is not well captured by frequency of use. It is better understood as **removability**: what becomes harder, slower, or less reliable when the scaffold is unavailable.[5][7]

In engineering terms, this is a question of **resilience**: how the workflow behaves under interruption.[7] One reason the effects can feel surprisingly broad is that the workflow has adapted toward “retrieve on demand” rather than “retain internally,” a shift documented in earlier work on access to external information.[12]

Dependency also has thresholds. Early use can be elective and occasional. But once a scaffold becomes the default way to bridge ordinary uncertainty, a user adapts their workflow around it. The surrounding environment adapts too. Speed and fluency norms rise, and expectations shift toward immediate coherence. Evidence from workplace settings suggests generative AI can raise productivity and consistency in routine tasks.[1] In that setting, opting out is no longer neutral. It is often experienced as falling behind.

The point is not that dependency is always bad. The point is that it changes what counts as normal competence. When the baseline workflow includes an intermediate cognition layer, skills and standards reorganize around its presence. The next question is what a reasonable response looks like—personally, organizationally, and in design—if we want the benefits without collapsing judgment into fluency.[18]


## 8) Dependency as the Outcome: When the Scaffold Becomes Assumed

When the scaffold becomes assumed, the **intermediate cognition layer** shifts from a tool used occasionally to a default step that workflows and environments are built around. As expectations adapt to immediate coherence and low friction, dependency becomes a predictable outcome of routine scaffolding rather than an accidental side effect. In that setting, “appropriate reliance” is less a matter of trusting outputs than of maintaining **resilience** and **legibility**: the ability to continue framing and initiating work under interruption, and a clear sense of which cognitive steps have been delegated and when they need to be reclaimed.[1][5][7][12][15][18]


#korovamode


---

## Appendix A: Key Terms and Related Labels

- **Scaffold:** a support layer that makes work easier while it is present, and reveals its role when it is removed. *(Related: cognitive scaffolding; extended cognition.)*
- **Intermediate cognition layer:** the first pass supplied on demand—interpretation, framing, formulation, checking, and next-step selection. *(Related: cognitive offloading.)*
- **Originator mode → editor mode:** role shift from generating first frames to selecting and revising suggested options. *(Related: human–AI co-writing; post-editing; human-in-the-loop writing.)*
- **Offloading:** delegating intermediate cognition to an external system, not merely retrieving facts. *(Related: transactive memory; externalized cognition.)*
- **Mediation:** the scaffold’s steering effect on what feels salient, safe, or “complete,” via pre-structured options and fluent outputs. *(Related: choice architecture; decision support.)*
- **Suggested options:** proposed framings, drafts, and next steps that structure choice. *(Related: defaults; nudges; decision support.)*
- **Drift:** gradual change in habits and capacities under routine scaffolding. *(Subtypes: interpretation drift; formulation drift; verification drift.)*
- **Verification gradient:** verification pressure rises as tasks become more ambiguous, unfamiliar, or high-stakes while checking costs rise.
- **Fluency-as-understanding:** treating smooth language as evidence of sound reasoning.
- **Confidence laundering:** uncertainty converted into crisp prose that carries a completion signal without verification.
- **Removability:** dependency revealed by interruption or constraint—what becomes visible when the scaffold is unavailable. *(Related: robustness under interruption; resilience.)*
- **Appropriate reliance / trust calibration:** repeated, context-sensitive calibration of when to accept, revise, verify independently, or step outside the offered frame.
- **Cognitive debt:** a term used by some researchers for the accumulated cost of repeatedly skipping internal first-pass work that would otherwise build capacity.[9]


## Appendix B: FAQ

### Do AI assistants reduce critical thinking?
They can, when a first-pass scaffold becomes the default response to uncertainty and the “thinking-to-start” step is practiced less often.[10]

### What is cognitive offloading with AI tools?
It is delegating intermediate steps—interpretation, framing, formulation, checking, and next-step selection—to an external system.[7]

### What is automation bias in large language models?
It is the tendency to accept system outputs too readily, especially when they are fluent, confident, and presented as “completion.”[17][20][16]

### How do AI writing assistants change how people write?
They can shift workflows from originator mode to editor mode, where revision remains visible but first framing is supplied externally.[8]

### What is “appropriate reliance”?
It is calibrated use: knowing when to accept, revise, verify independently, or step outside the offered frame.[15]

### What is cognitive debt from AI assistance?
It is a way some researchers describe the accumulated cost of repeatedly skipping internal first-pass work that would otherwise build capacity.[9]


## Endnotes

[1] Erik Brynjolfsson, Danielle Li, and Lindsey Raymond, “Generative AI at Work,” *The Quarterly Journal of Economics* 140, no. 2 (May 2025): 889–942. https://doi.org/10.1093/qje/qjae044

[2] Anjuli Kannan et al., “Smart Reply: Automated Response Suggestion for Email,” in *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (KDD ’16), 2016, 955–964. https://doi.org/10.1145/2939672.2939801

[3] Sumit Asthana, Sagi Hilleli, Pengcheng He, and Aaron Halfaker, “Summaries, Highlights, and Action Items: Design, Implementation and Evaluation of an LLM-powered Meeting Recap System,” *Proceedings of the ACM on Human-Computer Interaction* 9, no. 2 (CSCW) (April 2025), Article CSCW176. https://doi.org/10.1145/3711074

[4] Gaole He, Gianluca Demartini, and Ujwal Gadiraju, “Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant,” arXiv (2025). https://arxiv.org/abs/2502.01390 (related DOI: https://doi.org/10.1145/3706598.3713218)

[5] Andy Clark and David J. Chalmers, “The Extended Mind,” *Analysis* 58, no. 1 (1998): 7–19. https://doi.org/10.1093/analys/58.1.7

[6] Amos Tversky and Daniel Kahneman, “The Framing of Decisions and the Psychology of Choice,” *Science* 211, no. 4481 (1981): 453–458. https://doi.org/10.1126/science.7455683

[7] Evan F. Risko and Sam J. Gilbert, “Cognitive Offloading,” *Trends in Cognitive Sciences* 20, no. 9 (2016): 676–688. https://doi.org/10.1016/j.tics.2016.07.002

[8] Azmine Toushik Wasi, Mst Rafia Islam, and Raima Islam, “LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning,” In2Writing ’24 / arXiv:2404.00027 (2024). https://arxiv.org/abs/2404.00027

[9] Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein, and Pattie Maes, “Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task,” arXiv (2025). https://arxiv.org/abs/2506.08872

[10] Hao-Ping (Hank) Lee and Advait Sarkar, “The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers,” in *Proceedings of CHI 2025: CHI Conference on Human Factors in Computing Systems* (2025). https://doi.org/10.1145/3706598.3713778

[11] Shakked Noy and Whitney Zhang, “Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence,” *Science* 381, no. 6654 (2023): 187–192. https://doi.org/10.1126/science.adh2586

[12] Betsy Sparrow, Jenny Liu, and Daniel M. Wegner, “Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips,” *Science* 333, no. 6043 (2011): 776–778. https://doi.org/10.1126/science.1207745

[13] Adrian F. Ward, Kristen Duke, Ayelet Gneezy, and Maarten W. Bos, “Brain Drain: The Mere Presence of One’s Own Smartphone Reduces Available Cognitive Capacity,” *Journal of the Association for Consumer Research* 2, no. 2 (2017): 140–154. https://doi.org/10.1086/691462

[14] Korovamode K., “The New Machinery of Persuasion: Generative AI, Influence Architecture, and the Quiet Steering of Thought” (manuscript, 2025), PhilPapers (added November 26, 2025): https://philpapers.org/rec/KTNMIV. DOI: 10.5281/zenodo.17721122. Accessed December 28, 2025.

[15] John D. Lee and Katrina A. See, “Trust in Automation: Designing for Appropriate Reliance,” *Human Factors* 46, no. 1 (2004): 50–80. https://doi.org/10.1518/hfes.46.1.50_30392

[16] Giuseppe Romeo and Daniela Conti, “Exploring Automation Bias in Human–AI Collaboration: A Review and Implications for Explainable AI,” *AI & Society* (2025). https://doi.org/10.1007/s00146-025-02422-7

[17] Raja Parasuraman and Victor Riley, “Humans and Automation: Use, Misuse, Disuse, Abuse,” *Human Factors* 39, no. 2 (1997): 230–253. https://doi.org/10.1518/001872097778543886

[18] Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo, and Olga Russakovsky, “Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies,” arXiv (2025). https://arxiv.org/abs/2502.08554

[19] Raja Parasuraman and Dietrich H. Manzey, “Complacency and Bias in Human Use of Automation: An Attentional Integration,” *Human Factors* 52, no. 3 (2010): 381–410. https://doi.org/10.1177/0018720810376055

[20] Rohan Khera, Melissa A. Simon, and Joseph S. Ross, “Automation Bias and Assistive AI: Risk of Harm From AI-Driven Clinical Decision Support,” *JAMA* 330, no. 23 (2023): 2255–2257. https://doi.org/10.1001/jama.2023.22557
