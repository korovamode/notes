# Le Soi Augmenté  
## Échafaudages de l’IA, Externalisation Cognitive et Dérive vers la Dépendance

Korovamode | 28 décembre 2025


**Résumé**  
*Le Soi Augmenté* soutient que les assistants d’IA du quotidien deviennent des échafaudages cognitifs, façonnant non seulement la formulation finale, mais aussi l’interprétation précoce, le cadrage et la sélection des étapes suivantes. À mesure que cette « couche de cognition intermédiaire » devient routinière, les utilisateurs peuvent passer d’un mode d’initiateur à un mode d’éditeur — en sélectionnant et en révisant des options suggérées plutôt qu’en générant des cadres initiaux. L’essai distingue l’externalisation (ce qui est délégué) de la médiation (la manière dont les suggestions orientent l’attention et le sentiment d’achèvement), et avertit que la fluidité des productions peut réduire le recadrage et la vérification, produisant une dérive progressive. La dépendance devient la plus visible en cas d’interruption ; l’essai propose donc la *retirabilité* et la résilience des workflows comme guides pratiques d’un recours approprié.

**Mots-clés**  
assistants d’IA ; échafaudage cognitif ; couche de cognition intermédiaire ; externalisation cognitive ; mode initiateur ; mode éditeur ; médiation ; options suggérées ; effets de cadrage ; biais d’automatisation ; fluidité-comme-compréhension ; blanchiment de la confiance ; déqualification ; atrophie des compétences ; dérive ; dépendance ; retirabilité ; résilience ; recours approprié ; calibration de la confiance. 


## 1) L’Assistance Quotidienne par l’IA est Désormais la Norme

Les assistants d’IA sont de plus en plus utilisés dans le travail quotidien pour des tâches qui exigeaient auparavant une délibération active. Ils peuvent produire rapidement un langage exploitable — brouillons, ajustements de ton et réponses suggérées que l’utilisateur modifie avant de poursuivre.[1][2] Ils peuvent également réduire l’effort lié au traitement de l’information en résumant des textes longs, en clarifiant des termes inconnus et en transformant une intention vague en un plan simple.[3][4] À mesure que ce type de soutien de premier passage devient routinier, il modifie la manière dont le travail commence : on passe de la production d’un cadre initial à la sélection et à la révision d’options suggérées.

Cela s’inscrit dans une histoire plus longue de l’amélioration du travail par les outils. Les correcteurs orthographiques et grammaticaux ont réduit les erreurs de surface qui consommaient autrefois l’attention. Les modèles et l’autocomplétion ont réduit les formulations répétitives et le travail de mise en forme. Les règles, filtres et macros ont supprimé des étapes que l’utilisateur effectuait auparavant manuellement. Chaque étape a rendu le travail plus fluide, plus rapide et plus cohérent.

Ce qui change aujourd’hui, c’est le niveau auquel l’assistance opère. Elle n’aide plus seulement à polir le résultat. Elle participe de plus en plus aux premiers mouvements : en proposant une interprétation de ce qu’un document dit, en suggérant des implications possibles, en offrant un cadrage initial pour une tâche et en générant des étapes suivantes plausibles.

Une **couche de cognition intermédiaire** est désormais disponible à la demande. Elle se situe entre l’entrée brute de l’utilisateur et la sortie finale, fournissant un passage externe rapide qui transforme l’ambiguïté en quelque chose d’exploitable — un plan, un brouillon, une liste d’actions ou un cadrage provisoire.

Cela rend l’**externalisation cognitive** plus facile dès les premières étapes du workflow. Un utilisateur peut déléguer les premiers mouvements avant même de savoir ce qu’il pense ou comment il souhaite procéder. Le résultat délégué n’est pas une décision finale ; c’est une prise provisoire que l’utilisateur peut accepter, rejeter ou remodeler.

Dans ce rôle, l’assistant fournit un **premier passage** : la première version exploitable du raisonnement et du langage. Il offre une interprétation initiale et une direction de départ, ainsi qu’un brouillon approximatif qui donne à l’utilisateur quelque chose de concret à réviser.

Lorsque cette couche fournit les premiers mouvements, elle fonctionne comme un **échafaudage** — une couche de soutien qui facilite le travail tant qu’elle est présente, et qui révèle son rôle lorsqu’elle est retirée.[5] L’utilisateur fait toujours le travail, mais ce soutien modifie ce que signifie « commencer ». Il réduit la friction liée au démarrage.

L’**échafaudage** ne se contente pas de fournir de l’information. Il produit des **options suggérées** de sens et d’action — des cadrages candidats, des postures implicites et des brouillons dans une voix donnée.[6] Il ne s’agit pas de réponses à accepter telles quelles, mais d’options à réviser.

Un exemple simple illustre ce schéma. Un utilisateur reçoit un message dense ou délicat. L’assistant propose une réponse dans un style approprié.[2] L’utilisateur effectue des modifications, puis envoie le message. Le résultat peut paraître fluide même lorsque le travail interprétatif initial a été partiellement externalisé.[7]

Pour les étudiants, le même mécanisme apparaît dans la rédaction : reformulation du sujet, suggestions de plan, paragraphes de brouillon et aide à la révision.[8] L’étudiant révise toujours, mais la première direction exploitable et le premier langage sont souvent fournis par l’assistant.[9]

Lorsque cette couche est peu coûteuse et toujours disponible, elle peut devenir la manière par défaut de combler de petits écarts de compréhension et d’exécution. Ces écarts sont rarement spectaculaires ; ils se manifestent dans des moments ordinaires d’incertitude où l’utilisateur s’arrêterait autrement pour élaborer une première interprétation et un langage de départ. Avec le temps, le workflow peut passer de la génération d’un cadre à la sélection parmi des cadres prêts à l’emploi.[7]


## 2) Le Changement de Rôle Pratique : Mode Initiateur → Mode Éditeur

À mesure que ce type d’assistance devient routinier, il modifie la manière dont le travail est amorcé et stabilisé. Le changement tient au fait qu’un échafaudage externe fournit de plus en plus souvent la première direction cohérente, et pas seulement une formulation plus rapide. Le rôle de l’utilisateur peut passer de la production d’un premier passage à la sélection et à la révision d’un premier passage fourni.[7][8] Des enquêtes auprès de travailleurs du savoir rapportent des diminutions auto-déclarées de l’effort cognitif ainsi que des effets de confiance lors de l’usage d’outils d’IA générative.[10]

En **mode initiateur**, l’utilisateur commence dans l’incertitude et génère un premier cadre en interne. Il décide ce qu’est la tâche et ce qui y importe. Il choisit également une posture à adopter. Puis il produit un langage initial qui rend le travail suffisamment concret pour être révisé.

En **mode éditeur**, le premier cadre et la première formulation arrivent souvent sous forme d’**options suggérées**. L’outil propose une interprétation, un plan ou un brouillon dans un style approprié. L’utilisateur révise toujours. Mais les premiers mouvements — ce que cela signifie et comment commencer — ne sont plus systématiquement générés à partir de zéro.[7]

Certaines recherches décrivent ce schéma comme de la **co-écriture humain–IA** et comme du **post-édition**. L’utilisateur oriente le résultat par la révision. L’outil fournit un premier passage que l’utilisateur remodèle.[8]

Ce déplacement peut être difficile à percevoir parce que la révision reste visible. L’utilisateur continue de réviser — en modifiant des phrases, en corrigeant des détails et en ajustant le ton. Ce qui devient moins visible est le point de départ. Le workflow commence à partir d’un premier passage fourni de l’extérieur, qui porte déjà un cadre et peut emprunter la confiance de l’outil quant à ce qu’il convient de faire ensuite.[10][7]

Il existe de réels bénéfices. Le mode éditeur peut réduire la friction et accélérer l’écriture routinière. Il peut aussi rendre des tâches inconnues plus abordables en abaissant le coût d’atteindre un premier brouillon exploitable. Des expériences contrôlées sur des tâches d’écriture professionnelle rapportent des gains mesurables de productivité, avec des effets variables mais souvent positifs sur la qualité des productions.[11] Dans les organisations, ces workflows peuvent accroître la cohérence des communications routinières et aider les personnes à atteindre plus rapidement un niveau acceptable.[1]

Mais ce changement de rôle modifie aussi ce qui est pratiqué. En mode initiateur, produire un premier passage est un terrain d’entraînement pour le cadrage et la formulation. C’est aussi un entraînement à l’auto-vérification tant que le travail reste instable. En mode éditeur, ces capacités sont mobilisées de manière intermittente. L’échafaudage peut fournir la structure initiale dès que l’utilisateur hésite.[7] Avec le temps, l’utilisateur peut devenir plus fluide dans la révision d’options que dans la génération de premières options.[7]

Ce changement de rôle est porté par l’**externalisation** et la **médiation**. L’externalisation délègue la cognition intermédiaire à l’échafaudage. La médiation désigne la manière dont les options de l’échafaudage façonnent ce qui paraît saillant et sûr à faire ensuite.[7]


## 3) Externalisation : Ce Qui Est Délégué

Le passage au mode éditeur est alimenté par l’**externalisation**. Externaliser ne signifie pas seulement stocker l’information ailleurs ou récupérer des faits plus rapidement. Cela signifie déléguer la **cognition intermédiaire**. Il s’agit du premier passage d’interprétation, de cadrage, de formulation et de vérification qui transforme l’incertitude en une étape suivante exploitable.[7]

L’externalisation peut être difficile à repérer parce que l’utilisateur reste « dans la boucle ». Il décide toujours quoi conserver, quoi rejeter et quoi envoyer. Mais l’échafaudage fournit de plus en plus souvent la structure initiale : une lecture provisoire de ce que le texte semble dire, de ce qui paraît important, de la posture à adopter et un langage qui compte déjà comme un premier brouillon acceptable.

Ce qui est délégué tend à se regrouper en quatre zones. D’abord l’**interprétation** : un compte rendu rapide de ce que quelque chose signifie en contexte. Ensuite la **formulation** : transformer une intention en langage utilisable. Puis la **vérification** : plausibilité, complétude et vérifications de base. Enfin la **sélection de l’étape suivante** : traduire une situation en plan, recommandation ou réponse. Lorsque ces éléments sont fournis à la demande, le travail commence souvent déjà stabilisé.[7]

Cela importe parce que le premier passage n’est pas seulement une étape de production. C’est aussi l’endroit où certaines capacités clés sont exercées. Produire un cadre initial mobilise la capacité à tolérer l’ambiguïté et à décider de ce qui est saillant. Cela mobilise aussi la capacité à générer du langage avant que quoi que ce soit ne soit stabilisé. Lorsque l’externalisation devient la réponse par défaut à l’hésitation, ces capacités sont pratiquées moins souvent et de manière plus superficielle.[7]

L’externalisation peut également modifier les stratégies de mémoire. Lorsque l’échafaudage fournit de manière fiable un premier passage, l’utilisateur a moins de raisons de conserver les étapes intermédiaires en interne. Ces étapes deviennent quelque chose que l’on récupère à la demande.[12] L’externalisation peut aussi modifier l’attention. Des travaux connexes suggèrent que la simple présence d’une aide externe peut réduire la capacité cognitive disponible, même lorsque l’utilisateur ne l’emploie pas activement.[13] Ce n’est pas automatiquement nuisible, mais cela modifie ce que l’utilisateur peut faire rapidement et avec assurance sans l’échafaudage.[7]


## 4) Médiation : Comment l’Échafaudage Oriente le Travail

L’externalisation décrit ce qui est délégué. La **médiation** décrit ce qui arrive ensuite au travail. L’échafaudage intervient en fournissant une manière pré-structurée de voir la situation et d’agir en son sein. Il fournit une interprétation. Il fournit un cadre. Il fournit une étape suivante prête à l’emploi dans un langage fluide.

Cela importe parce que les **options suggérées** ne sont pas des contenants neutres. Chaque option arrive avec une posture implicite. Elle implique ce qui est saillant et ce qui constitue le problème. Elle implique quels risques comptent. Elle implique quel type de réponse est approprié. Même lorsque l’utilisateur révise, le point de départ restreint l’attention. Un cadrage devient immédiatement exploitable. D’autres cadrages deviennent moins accessibles.[6]

C’est la logique de l’**architecture du choix** et de l’**aide à la décision**. L’ensemble des options constitue déjà une forme d’orientation.[6][14]

La médiation est souvent vécue comme de la clarté. L’utilisateur passe de l’ambiguïté à une trajectoire cohérente : ce que cela signifie, ce qui paraît saillant et quoi faire ensuite. Cela peut être réellement utile, surtout sous contrainte de temps ou en situation d’inexpérience. Des enquêtes auprès de travailleurs du savoir rapportent des diminutions auto-déclarées de l’effort cognitif ainsi que des effets de confiance lors de l’usage d’outils d’IA générative.[10] Lorsque la cohérence arrive rapidement et que l’effort diminue, les signaux internes qui déclencheraient habituellement un recadrage ou une vérification plus approfondie peuvent devenir moins susceptibles de s’activer.[10][15][16]

Le risque central n’est pas que les personnes « fassent confiance à l’IA » au sens global. Le risque est une **dépendance mal calibrée**. C’est une forme de **sur-dépendance** dans des contextes où les productions de l’échafaudage ne sont pas de manière fiable appropriées. Cela inclut le fait de traiter une prose confiante comme un substitut de justesse. Cela inclut le fait de traiter des résumés plausibles comme s’ils étaient vérifiés. Cela inclut le fait de laisser un plan suggéré fixer la posture de risque par défaut sans recadrage.[15][16]

Les analyses classiques de l’automatisation décrivent toute une famille de modes d’échec autour de ce problème de calibration. Le **mauvais usage** correspond à une sur-dépendance. Le **sous-usage** correspond à une non-utilisation même lorsque le système serait utile. L’**abus** renvoie à des conceptions ou des incitations qui poussent la dépendance au-delà de ce qui est justifié. Les détails varient selon les domaines, mais le problème sous-jacent est le même : la médiation modifie la manière dont le jugement humain est distribué au sein d’un système.[17]

Des travaux récents spécifiques aux modèles de langage soulignent le même problème de calibration et explorent des interventions. Parmi elles : exposer l’incertitude, mettre en évidence les sources, signaler les incohérences et rendre les limites visibles au point d’usage. Il ne s’agit pas de caractéristiques cosmétiques. Ce sont des tentatives de remodeler la médiation afin que la « prose acceptable » ne se confonde pas avec un « raisonnement acceptable ».[18]


## 5) Dérive : Comment les Habitudes et les Capacités Évoluent

Avec le temps, l’échafaudage routinier peut produire une **dérive**. La dérive n’est ni une défaillance unique ni un effondrement spectaculaire. C’est un changement progressif de ce qui est pratiqué, de ce qui devient automatique et de ce qui commence à demander plus d’effort. Lorsque le premier passage est fréquemment fourni de l’extérieur, les capacités internes qui l’auraient produit sont exercées moins souvent. Elles peuvent alors s’amincir en conséquence. Dans le discours plus large, on parle souvent de **déqualification** ou d’**atrophie des compétences**. La dérive désigne le même processus au niveau du workflow quotidien.[7]

Trois formes de dérive sont particulièrement courantes : la **dérive interprétative**, la **dérive de formulation** et la **dérive de vérification**.

La **dérive interprétative** est la tendance à s’appuyer sur des résumés et des cadrages externes plutôt que de construire un compte rendu interne. L’utilisateur devient plus rapide pour accepter ou ajuster légèrement une interprétation proposée que pour en générer une première à partir de l’ambiguïté. Avec le temps, l’étape de « mise en sens pour commencer » est plus souvent externalisée que pratiquée.[7]

La **dérive de formulation** est la tendance à traiter le langage comme quelque chose que l’on sélectionne et ajuste plutôt que comme quelque chose que l’on génère. L’utilisateur devient fluide dans la révision — ton, registre, structure — tandis que la capacité à produire un premier brouillon en situation d’incertitude est pratiquée moins souvent. L’utilisateur peut continuer à bien écrire, mais le chemin passe de plus en plus par des options fournies par l’échafaudage.[7]

La **dérive de vérification** est la tendance à vérifier moins, parce que la production arrive fluide et complète. L’utilisateur apprend que l’échafaudage fournit généralement quelque chose d’exploitable. Les déclencheurs internes de scepticisme s’affaiblissent, surtout dans des contextes à faible enjeu et à forte fréquence. Cela recoupe le **biais d’automatisation** : accepter trop facilement la sortie d’un système parce qu’elle se présente comme une réponse finalisée.[19][17] Dans le cas des modèles de langage, le risque est amplifié par des productions qui peuvent paraître fluides et convaincantes même lorsqu’elles sont erronées.[18][16]

Ces dérives sont amplifiées par des dynamiques cognitives bien connues. L’une est la **fluidité-comme-compréhension** : traiter un langage fluide comme une preuve que le raisonnement sous-jacent est solide. Une autre est le **blanchiment de la confiance** : l’incertitude est transformée en une prose nette qui porte un signal d’achèvement. Le travail peut sembler résolu alors qu’il n’a pas été vérifié.[18]

Ces dérives ne sont pas symétriques. La révision peut rester forte tandis que l’initiation s’affaiblit. Un utilisateur peut conserver une grande capacité à améliorer le langage tout en étant moins entraîné à générer des cadres initiaux et à effectuer des vérifications indépendantes. Le résultat n’est pas l’ignorance. C’est une repondération de ce que le workflow entraîne.


## 6) Agentivité : Options, Cadres et le Rétrécissement Subtil du Choix

L’échafaudage est souvent décrit comme une « aide », mais son influence s’exerce par le **cadrage**. Il n’accélère pas seulement l’exécution. Il propose ce qu’est la situation et ce qui y importe. Il implique aussi quel type de réponse est approprié. Même lorsque l’utilisateur révise, le cadre de départ peut façonner ce qui paraît pensable, urgent et digne d’être fait en premier.[6]

C’est pourquoi l’expression « options suggérées » n’est pas une description neutre. Les options arrivent avec des valeurs par défaut. Elles portent une posture de risque. Elles portent un ton. Elles portent une structure de priorités implicite. Le rôle de l’utilisateur peut se déplacer vers la sélection parmi des trajectoires préformées plutôt que vers la délibération à partir d’une incertitude ouverte. L’agentivité peut rester présente, mais il devient plus facile de confondre acceptation-avec-édition et jugement indépendant. Une manière de nommer ce déplacement est la **dépendance épistémique** : le premier sens de « ce que c’est » pour l’utilisateur est de plus en plus fourni plutôt que construit.[6]

Le problème n’est pas que l’échafaudage supprime le choix. Le problème est qu’il peut comprimer le moment où l’utilisateur générerait normalement un cadre. Lorsque la première interprétation cohérente arrive immédiatement, les cadrages alternatifs doivent être produits activement plutôt que découverts passivement. Avec le temps, « commencer à partir d’options » peut devenir l’attente implicite de la manière dont la pensée débute.[6][7]

L’agentivité en mode éditeur devient moins automatique. Le premier cadre exploitable arrive déjà stabilisé, de sorte que l’auteur se voit souvent réduit à une question plus étroite : faut-il adopter le cadre proposé, et jusqu’où s’en écarter. Ce qui tend à disparaître est le bref intervalle pendant lequel des cadrages alternatifs seraient autrement générés. Lorsque cet intervalle manque, « partir d’options » peut fonctionner comme un défaut, même si la révision reste visible.[6][7]

C’est également ici que le **recours approprié** devient opérationnel plutôt qu’abstrait. Le recours est moins un jugement global de confiance qu’une calibration répétée de ce que l’échafaudage est autorisé à stabiliser — interprétation, posture, posture de risque ou étapes suivantes — avant qu’un recadrage indépendant n’ait lieu. Des caractéristiques de conception qui exposent l’incertitude, révèlent les sources ou signalent les incohérences peuvent soutenir cette calibration, mais elles n’éliminent pas la dépendance sous-jacente à une appropriation active du cadrage.[18][15][16]


## 7) La Dépendance Devient Lisible : Interruption et Contrainte

La dépendance est souvent la plus visible en situation d’interruption ou de contrainte. Lorsque l’accès change — indisponibilité temporaire, niveaux d’abonnement, limites de taux, filtrage par politique ou restrictions contextuelles — les effets apparaissent généralement comme des altérations distribuées plutôt que comme un effondrement spectaculaire. Le travail continue, mais de manière inégale. L’utilisateur ressent une friction à des endroits qui étaient auparavant discrètement pris en charge par l’échafaudage.[7]

Les altérations se situent généralement en amont. **Commencer** devient plus difficile parce qu’un premier cadre cohérent n’est plus fourni à la demande. La **formulation** ralentit parce que le langage initial n’est plus disponible comme option suggérée à réviser. La **vérification** s’amenuise parce que la validation n’arrive plus intégrée à des signaux d’achèvement et à une prose confiante. L’**incertitude** augmente parce que le workflow n’inclut plus ce passage externe rapide qui stabilisait le sens et les étapes suivantes.[7]

La dépendance n’est pas bien mesurée par la fréquence d’usage. Elle est mieux comprise en termes de **retirabilité** : ce qui devient plus difficile, plus lent ou moins fiable lorsque l’échafaudage n’est pas disponible.[5][7]

En termes d’ingénierie, il s’agit d’une question de **résilience** : la manière dont le workflow se comporte sous interruption.[7] L’une des raisons pour lesquelles les effets peuvent sembler étonnamment étendus est que le workflow s’est adapté vers un modèle de « récupération à la demande » plutôt que de « rétention interne », un déplacement documenté par des travaux antérieurs sur l’accès à l’information externe.[12]

La dépendance comporte également des seuils. Les premiers usages peuvent être électifs et occasionnels. Mais une fois qu’un échafaudage devient la manière par défaut de combler l’incertitude ordinaire, l’utilisateur adapte son workflow autour de lui. L’environnement s’adapte aussi. Les normes de vitesse et de fluidité augmentent, et les attentes se déplacent vers une cohérence immédiate. Des données issues des environnements de travail suggèrent que l’IA générative peut accroître la productivité et la cohérence dans les tâches routinières.[1] Dans ce contexte, se retirer n’est plus neutre. Cela est souvent vécu comme un retard.

Le propos n’est pas de dire que la dépendance est toujours mauvaise. Le propos est qu’elle modifie ce qui compte comme une compétence normale. Lorsque le workflow de base inclut une couche de cognition intermédiaire, les compétences et les standards se réorganisent autour de sa présence. La question suivante est alors de savoir à quoi ressemble une réponse raisonnable — sur le plan personnel, organisationnel et de la conception — si l’on veut les bénéfices sans réduire le jugement à la seule fluidité.[18]


## 8) La Dépendance comme Résultat : Lorsque l’Échafaudage Devient Implicite

Lorsque l’échafaudage devient implicite, la **couche de cognition intermédiaire** passe d’un outil utilisé occasionnellement à une étape par défaut autour de laquelle les workflows et les environnements sont construits. À mesure que les attentes s’adaptent à une cohérence immédiate et à une faible friction, la dépendance devient un résultat prévisible de l’échafaudage routinier plutôt qu’un effet secondaire accidentel. Dans ce contexte, le « recours approprié » est moins une question de confiance dans les productions qu’une question de **résilience** et de **lisibilité** : la capacité à continuer à cadrer et à initier le travail en situation d’interruption, et une compréhension claire des étapes cognitives qui ont été déléguées et des moments où elles doivent être reprises.[1][5][7][12][15][18]


#korovamode


---

## Annexe A : Termes Clés et Étiquettes Associées

- **Échafaudage (Scaffold)** : couche de soutien qui facilite le travail tant qu’elle est présente, et révèle son rôle lorsqu’elle est retirée. *(Associé : échafaudage cognitif ; cognition étendue.)*
- **Couche de cognition intermédiaire** : premier passage fourni à la demande — interprétation, cadrage, formulation, vérification et sélection de l’étape suivante. *(Associé : externalisation cognitive.)*
- **Mode initiateur → mode éditeur** : déplacement du rôle, de la génération de cadres initiaux vers la sélection et la révision d’options suggérées. *(Associé : co-écriture humain–IA ; post-édition ; écriture avec humain dans la boucle.)*
- **Externalisation (Offloading)** : délégation de la cognition intermédiaire à un système externe, et non simple récupération de faits. *(Associé : mémoire transactionnelle ; cognition externalisée.)*
- **Médiation** : effet d’orientation de l’échafaudage sur ce qui paraît saillant, sûr ou « complet », via des options pré-structurées et des productions fluides. *(Associé : architecture du choix ; aide à la décision.)*
- **Options suggérées** : cadrages, brouillons et étapes suivantes proposés qui structurent le choix. *(Associé : valeurs par défaut ; nudges ; aide à la décision.)*
- **Dérive** : modification progressive des habitudes et des capacités sous l’effet d’un échafaudage routinier. *(Sous-types : dérive interprétative ; dérive de formulation ; dérive de vérification.)*
- **Gradient de vérification** : la pression de vérification augmente à mesure que les tâches deviennent plus ambiguës, inhabituelles ou à forts enjeux, tandis que le coût de la vérification augmente.
- **Fluidité-comme-compréhension** : tendance à traiter un langage fluide comme une preuve de raisonnement solide.
- **Blanchiment de la confiance** : transformation de l’incertitude en une prose nette qui porte un signal d’achèvement sans vérification.
- **Retirabilité (Removability)** : dépendance révélée par l’interruption ou la contrainte — ce qui devient visible lorsque l’échafaudage est indisponible. *(Associé : robustesse sous interruption ; résilience.)*
- **Recours approprié / calibration de la confiance** : calibration répétée et contextuelle de quand accepter, réviser, vérifier indépendamment ou sortir du cadre proposé.
- **Dette cognitive** : terme utilisé par certains chercheurs pour désigner le coût cumulé du fait de sauter de manière répétée le travail interne de premier passage qui construirait autrement des capacités.[9]


## Annexe B : FAQ

### Les assistants d’IA réduisent-ils la pensée critique ?
Ils peuvent le faire lorsque l’échafaudage de premier passage devient la réponse par défaut à l’incertitude et que l’étape de « penser pour commencer » est pratiquée moins souvent.[10]

### Qu’est-ce que l’externalisation cognitive avec les outils d’IA ?
Il s’agit de déléguer des étapes intermédiaires — interprétation, cadrage, formulation, vérification et sélection de l’étape suivante — à un système externe.[7]

### Qu’est-ce que le biais d’automatisation dans les grands modèles de langage ?
C’est la tendance à accepter trop facilement les productions du système, surtout lorsqu’elles sont fluides, confiantes et présentées comme « complètes ».[17][20][16]

### Comment les assistants d’écriture par IA modifient-ils la manière d’écrire ?
Ils peuvent déplacer les workflows du mode initiateur vers le mode éditeur, où la révision reste visible mais où le cadrage initial est fourni de l’extérieur.[8]

### Qu’est-ce que le « recours approprié » ?
C’est un usage calibré : savoir quand accepter, réviser, vérifier de manière indépendante ou sortir du cadre proposé.[15]

### Qu’est-ce que la dette cognitive liée à l’assistance par IA ?
C’est une manière, pour certains chercheurs, de décrire le coût cumulé du fait de sauter de manière répétée le travail interne de premier passage qui construirait autrement des capacités.[9]


## Endnotes

## Notes de fin

[1] Erik Brynjolfsson, Danielle Li et Lindsey Raymond, « Generative AI at Work », *The Quarterly Journal of Economics* 140, no 2 (mai 2025) : 889–942. https://doi.org/10.1093/qje/qjae044

[2] Anjuli Kannan et al., « Smart Reply: Automated Response Suggestion for Email », dans *Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining* (KDD ’16), 2016, 955–964. https://doi.org/10.1145/2939672.2939801

[3] Sumit Asthana, Sagi Hilleli, Pengcheng He et Aaron Halfaker, « Summaries, Highlights, and Action Items: Design, Implementation and Evaluation of an LLM-powered Meeting Recap System », *Proceedings of the ACM on Human-Computer Interaction* 9, no 2 (CSCW) (avril 2025), article CSCW176. https://doi.org/10.1145/3711074

[4] Gaole He, Gianluca Demartini et Ujwal Gadiraju, « Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant », arXiv (2025). https://arxiv.org/abs/2502.01390 (DOI associé : https://doi.org/10.1145/3706598.3713218)

[5] Andy Clark et David J. Chalmers, « The Extended Mind », *Analysis* 58, no 1 (1998) : 7–19. https://doi.org/10.1093/analys/58.1.7

[6] Amos Tversky et Daniel Kahneman, « The Framing of Decisions and the Psychology of Choice », *Science* 211, no 4481 (1981) : 453–458. https://doi.org/10.1126/science.7455683

[7] Evan F. Risko et Sam J. Gilbert, « Cognitive Offloading », *Trends in Cognitive Sciences* 20, no 9 (2016) : 676–688. https://doi.org/10.1016/j.tics.2016.07.002

[8] Azmine Toushik Wasi, Mst Rafia Islam et Raima Islam, « LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning », In2Writing ’24 / arXiv:2404.00027 (2024). https://arxiv.org/abs/2404.00027

[9] Nataliya Kosmyna, Eugene Hauptmann, Ye Tong Yuan, Jessica Situ, Xian-Hao Liao, Ashly Vivian Beresnitzky, Iris Braunstein et Pattie Maes, « Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task », arXiv (2025). https://arxiv.org/abs/2506.08872

[10] Hao-Ping (Hank) Lee et Advait Sarkar, « The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers », dans *Proceedings of CHI 2025: CHI Conference on Human Factors in Computing Systems* (2025). https://doi.org/10.1145/3706598.3713778

[11] Shakked Noy et Whitney Zhang, « Experimental Evidence on the Productivity Effects of Generative Artificial Intelligence », *Science* 381, no 6654 (2023) : 187–192. https://doi.org/10.1126/science.adh2586

[12] Betsy Sparrow, Jenny Liu et Daniel M. Wegner, « Google Effects on Memory: Cognitive Consequences of Having Information at Our Fingertips », *Science* 333, no 6043 (2011) : 776–778. https://doi.org/10.1126/science.1207745

[13] Adrian F. Ward, Kristen Duke, Ayelet Gneezy et Maarten W. Bos, « Brain Drain: The Mere Presence of One’s Own Smartphone Reduces Available Cognitive Capacity », *Journal of the Association for Consumer Research* 2, no 2 (2017) : 140–154. https://doi.org/10.1086/691462

[14] Korovamode K., « The New Machinery of Persuasion: Generative AI, Influence Architecture, and the Quiet Steering of Thought » (manuscrit, 2025), PhilPapers (ajouté le 26 novembre 2025) : https://philpapers.org/rec/KTNMIV. DOI : 10.5281/zenodo.17721122. Consulté le 28 décembre 2025.

[15] John D. Lee et Katrina A. See, « Trust in Automation: Designing for Appropriate Reliance », *Human Factors* 46, no 1 (2004) : 50–80. https://doi.org/10.1518/hfes.46.1.50_30392

[16] Giuseppe Romeo et Daniela Conti, « Exploring Automation Bias in Human–AI Collaboration: A Review and Implications for Explainable AI », *AI & Society* (2025). https://doi.org/10.1007/s00146-025-02422-7

[17] Raja Parasuraman et Victor Riley, « Humans and Automation: Use, Misuse, Disuse, Abuse », *Human Factors* 39, no 2 (1997) : 230–253. https://doi.org/10.1518/001872097778543886

[18] Sunnie S. Y. Kim, Jennifer Wortman Vaughan, Q. Vera Liao, Tania Lombrozo et Olga Russakovsky, « Fostering Appropriate Reliance on Large Language Models: The Role of Explanations, Sources, and Inconsistencies », arXiv (2025). https://arxiv.org/abs/2502.08554

[19] Raja Parasuraman et Dietrich H. Manzey, « Complacency and Bias in Human Use of Automation: An Attentional Integration », *Human Factors* 52, no 3 (2010) : 381–410. https://doi.org/10.1177/0018720810376055

[20] Rohan Khera, Melissa A. Simon et Joseph S. Ross, « Automation Bias and Assistive AI: Risk of Harm From AI-Driven Clinical Decision Support », *JAMA* 330, no 23 (2023) : 2255–2257. https://doi.org/10.1001/jama.2023.22557
